WARNING:__main__:Running smaller batch size=4 for AlbertForMaskedLM, orig batch_size=8
cuda train AlbertForMaskedLM                   1.236x p=0.00
TIMING: entire_frame_compile:41.66975 backend_compile:39.92987
STATS: call_* op count: 439 | FakeTensorMode.__torch_dispatch__:94735 | FakeTensor.__torch_dispatch__:4535 | ProxyTorchDispatchMode.__torch_dispatch__:93155
Dynamo produced 1 graphs covering 439 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for AlbertForQuestionAnswering, orig batch_size=8
cuda train AlbertForQuestionAnswering          1.241x p=0.00
TIMING: entire_frame_compile:39.63603 backend_compile:37.91405
STATS: call_* op count: 439 | FakeTensorMode.__torch_dispatch__:93918 | FakeTensor.__torch_dispatch__:4414 | ProxyTorchDispatchMode.__torch_dispatch__:92121
Dynamo produced 1 graphs covering 439 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for AllenaiLongformerBase, orig batch_size=8
cuda train AllenaiLongformerBase               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 222, in make_nvfuser_fusion
    out = FusionInterpreter(gm).run(*nv_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 136, in run
    self.env[node] = self.run_node(node)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 184, in run_node
    return super().run_node(node)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 177, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 202, in output
    assert isinstance(
AssertionError: output from codegen has to be tensor type

While executing return [view, view_1, sub]
Original traceback:
None

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1848, in forward
    outputs = self.longformer(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1750, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1294, in forward
    is_global_attn = is_index_global_attn.flatten().any().item()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1326, in <graph break in forward>
    layer_outputs = layer_module(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1249, in forward
    self_attn_outputs = self.attention(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1185, in forward
    self_outputs = self.self(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 574, in forward
    attn_scores = self._sliding_chunks_query_key_matmul(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 821, in _sliding_chunks_query_key_matmul
    def _sliding_chunks_query_key_matmul(self, query: torch.Tensor, key: torch.Tensor, window_overlap: int):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2386, in debug_compiled_function
    return compiled_function(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2151, in forward
    fw_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 250, in nvfuser_execute
    fusion, unflatten_spec = make_nvfuser_fusion(gm, *nv_template_args)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 152, in make_nvfuser_fusion
    with FusionDefinition(fusion) as fd:
TypeError: __exit__(): incompatible function arguments. The following argument types are supported:
    1. (self: nvfuser._C.FusionDefinition, arg0: capsule, arg1: capsule, arg2: capsule) -> None

Invoked with: <nvfuser._C.FusionDefinition object at 0x7fec3eed18f0>, <class 'AssertionError'>, AssertionError('output from codegen has to be tensor type\n\nWhile executing return [view, view_1, sub]\nOriginal traceback:\nNone'), <traceback object at 0x7fec3eed3a40>
ERROR
WARNING:__main__:Running smaller batch size=4 for BartForCausalLM, orig batch_size=8
cuda train BartForCausalLM                     1.013x p=0.00
TIMING: entire_frame_compile:44.26699 backend_compile:41.0249
STATS: call_* op count: 482 | FakeTensorMode.__torch_dispatch__:99991 | FakeTensor.__torch_dispatch__:10301 | ProxyTorchDispatchMode.__torch_dispatch__:95419
Dynamo produced 14 graphs covering 482 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=2 for BartForConditionalGeneration, orig batch_size=4
cuda train BartForConditionalGeneration        0.472x p=0.00
TIMING: entire_frame_compile:104.7788 backend_compile:98.60662
STATS: call_* op count: 1258 | FakeTensorMode.__torch_dispatch__:251177 | FakeTensor.__torch_dispatch__:23330 | ProxyTorchDispatchMode.__torch_dispatch__:244009
Dynamo produced 1 graphs covering 1258 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for BertForMaskedLM, orig batch_size=32
cuda train BertForMaskedLM                     1.080x p=0.00
TIMING: entire_frame_compile:43.55832 backend_compile:40.73819
STATS: call_* op count: 370 | FakeTensorMode.__torch_dispatch__:102157 | FakeTensor.__torch_dispatch__:9539 | ProxyTorchDispatchMode.__torch_dispatch__:98476
Dynamo produced 1 graphs covering 370 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for BertForQuestionAnswering, orig batch_size=32
cuda train BertForQuestionAnswering            1.130x p=0.00
TIMING: entire_frame_compile:42.31194 backend_compile:39.49976
STATS: call_* op count: 377 | FakeTensorMode.__torch_dispatch__:101486 | FakeTensor.__torch_dispatch__:9418 | ProxyTorchDispatchMode.__torch_dispatch__:97432
Dynamo produced 1 graphs covering 377 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for BlenderbotForCausalLM, orig batch_size=32
cuda train BlenderbotForCausalLM               0.899x p=0.00
TIMING: entire_frame_compile:83.89339 backend_compile:77.55992
STATS: call_* op count: 934 | FakeTensorMode.__torch_dispatch__:194861 | FakeTensor.__torch_dispatch__:20204 | ProxyTorchDispatchMode.__torch_dispatch__:186512
Dynamo produced 26 graphs covering 934 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForCausalLM, orig batch_size=256
cuda train BlenderbotSmallForCausalLM          0.927x p=0.00
TIMING: entire_frame_compile:29.33575 backend_compile:26.90133
STATS: call_* op count: 327 | FakeTensorMode.__torch_dispatch__:68020 | FakeTensor.__torch_dispatch__:7004 | ProxyTorchDispatchMode.__torch_dispatch__:64708
Dynamo produced 10 graphs covering 327 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForConditionalGeneration, orig batch_size=128
cuda train BlenderbotSmallForConditionalGeneration  0.968x p=0.00
TIMING: entire_frame_compile:71.44695 backend_compile:66.97526
STATS: call_* op count: 840 | FakeTensorMode.__torch_dispatch__:168882 | FakeTensor.__torch_dispatch__:15669 | ProxyTorchDispatchMode.__torch_dispatch__:164273
Dynamo produced 1 graphs covering 840 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for CamemBert, orig batch_size=32
cuda train CamemBert                           1.071x p=0.00
TIMING: entire_frame_compile:44.9674 backend_compile:42.01004
STATS: call_* op count: 377 | FakeTensorMode.__torch_dispatch__:102402 | FakeTensor.__torch_dispatch__:9539 | ProxyTorchDispatchMode.__torch_dispatch__:98733
Dynamo produced 1 graphs covering 377 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for DebertaForMaskedLM, orig batch_size=32
cuda train DebertaForMaskedLM                  ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1072, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 975, in forward
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 983, in <graph break in forward>
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 445, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 419, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS111{( ceilDiv(( T8.size[0] * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS112{1}, iS134{( ceilDiv(( ceilDiv(( 32 * 32 ), 4) ), 128) )}, iS135{128}, iS133{4} ] with word size 4 not possible due to invalid stride. Domain: iS135{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=8 for DebertaForQuestionAnswering, orig batch_size=32
cuda train DebertaForQuestionAnswering         ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1401, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 975, in forward
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 983, in <graph break in forward>
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 445, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 419, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS111{( ceilDiv(( T8.size[0] * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS112{1}, iS134{( ceilDiv(( ceilDiv(( 32 * 32 ), 4) ), 128) )}, iS135{128}, iS133{4} ] with word size 4 not possible due to invalid stride. Domain: iS135{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=1 for DebertaV2ForMaskedLM, orig batch_size=8
cuda train DebertaV2ForMaskedLM                ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1171, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1075, in forward
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1083, in <graph break in forward>
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 488, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 452, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS109{( ceilDiv(( 1 * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS110{1}, iS132{( ceilDiv(( ceilDiv(( 32 * 32 ), 2) ), 128) )}, iS133{128}, iS131{2} ] with word size 2 not possible due to invalid stride. Domain: iS133{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=2 for DebertaV2ForQuestionAnswering, orig batch_size=8
cuda train DebertaV2ForQuestionAnswering       ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1501, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1075, in forward
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1083, in <graph break in forward>
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 488, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 452, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS111{( ceilDiv(( T8.size[0] * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS112{1}, iS134{( ceilDiv(( ceilDiv(( 32 * 32 ), 4) ), 128) )}, iS135{128}, iS133{4} ] with word size 4 not possible due to invalid stride. Domain: iS135{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=128 for DistilBertForMaskedLM, orig batch_size=256
WARNING:__main__:Sequence Length not defined for DistilBertForMaskedLM. Choosing 128 arbitrarily
cuda train DistilBertForMaskedLM               0.825x p=0.00
TIMING: entire_frame_compile:22.67555 backend_compile:21.29818
STATS: call_* op count: 206 | FakeTensorMode.__torch_dispatch__:53009 | FakeTensor.__torch_dispatch__:4753 | ProxyTorchDispatchMode.__torch_dispatch__:51631
Dynamo produced 1 graphs covering 206 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=256 for DistilBertForQuestionAnswering, orig batch_size=512
WARNING:__main__:Sequence Length not defined for DistilBertForQuestionAnswering. Choosing 128 arbitrarily
cuda train DistilBertForQuestionAnswering      0.801x p=0.00
TIMING: entire_frame_compile:22.38836 backend_compile:21.01726
STATS: call_* op count: 214 | FakeTensorMode.__torch_dispatch__:52543 | FakeTensor.__torch_dispatch__:4633 | ProxyTorchDispatchMode.__torch_dispatch__:50779
Dynamo produced 1 graphs covering 214 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for DistillGPT2, orig batch_size=32
cuda train DistillGPT2                         1.212x p=0.00
TIMING: entire_frame_compile:19.73544 backend_compile:18.30689
STATS: call_* op count: 330 | FakeTensorMode.__torch_dispatch__:44755 | FakeTensor.__torch_dispatch__:3435 | ProxyTorchDispatchMode.__torch_dispatch__:40393
Dynamo produced 1 graphs covering 330 ops with 4 graph breaks (3 unique)
If you want to use `ElectraForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=32 for ElectraForCausalLM, orig batch_size=64
cuda train ElectraForCausalLM                  1.051x p=0.00
TIMING: entire_frame_compile:45.90092 backend_compile:40.49783
STATS: call_* op count: 375 | FakeTensorMode.__torch_dispatch__:103173 | FakeTensor.__torch_dispatch__:9663 | ProxyTorchDispatchMode.__torch_dispatch__:99251
Dynamo produced 4 graphs covering 375 ops with 7 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=64 for ElectraForQuestionAnswering, orig batch_size=128
cuda train ElectraForQuestionAnswering         1.228x p=0.00
TIMING: entire_frame_compile:44.41452 backend_compile:41.5237
STATS: call_* op count: 378 | FakeTensorMode.__torch_dispatch__:102055 | FakeTensor.__torch_dispatch__:9509 | ProxyTorchDispatchMode.__torch_dispatch__:97955
Dynamo produced 1 graphs covering 378 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for GPT2ForSequenceClassification, orig batch_size=8
cuda train GPT2ForSequenceClassification       1.513x p=0.00
TIMING: entire_frame_compile:34.79714 backend_compile:32.3423
STATS: call_* op count: 644 | FakeTensorMode.__torch_dispatch__:85520 | FakeTensor.__torch_dispatch__:6700 | ProxyTorchDispatchMode.__torch_dispatch__:77094
Dynamo produced 1 graphs covering 644 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for GoogleFnet, orig batch_size=32
cuda train GoogleFnet                          1.506x p=0.00
TIMING: entire_frame_compile:25.16785 backend_compile:23.3875
STATS: call_* op count: 209 | FakeTensorMode.__torch_dispatch__:55588 | FakeTensor.__torch_dispatch__:4545 | ProxyTorchDispatchMode.__torch_dispatch__:56145
Dynamo produced 28 graphs covering 209 ops with 45 graph breaks (7 unique)
WARNING:__main__:Running smaller batch size=16 for LayoutLMForMaskedLM, orig batch_size=32
cuda train LayoutLMForMaskedLM                 1.080x p=0.00
TIMING: entire_frame_compile:46.21924 backend_compile:42.99584
STATS: call_* op count: 396 | FakeTensorMode.__torch_dispatch__:105115 | FakeTensor.__torch_dispatch__:10175 | ProxyTorchDispatchMode.__torch_dispatch__:100469
Dynamo produced 1 graphs covering 396 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for LayoutLMForSequenceClassification, orig batch_size=32
cuda train LayoutLMForSequenceClassification   1.130x p=0.00
TIMING: entire_frame_compile:45.97605 backend_compile:42.79971
STATS: call_* op count: 394 | FakeTensorMode.__torch_dispatch__:103898 | FakeTensor.__torch_dispatch__:10094 | ProxyTorchDispatchMode.__torch_dispatch__:99182
Dynamo produced 1 graphs covering 394 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for M2M100ForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for M2M100ForConditionalGeneration. Choosing 128 arbitrarily
cuda train M2M100ForConditionalGeneration      0.895x p=0.00
TIMING: entire_frame_compile:105.6196 backend_compile:98.65101
STATS: call_* op count: 1200 | FakeTensorMode.__torch_dispatch__:241947 | FakeTensor.__torch_dispatch__:24105 | ProxyTorchDispatchMode.__torch_dispatch__:231705
Dynamo produced 18 graphs covering 1200 ops with 17 graph breaks (7 unique)
WARNING:__main__:Running smaller batch size=4 for MBartForCausalLM, orig batch_size=8
cuda train MBartForCausalLM                    1.014x p=0.00
TIMING: entire_frame_compile:42.92902 backend_compile:39.47807
STATS: call_* op count: 481 | FakeTensorMode.__torch_dispatch__:100013 | FakeTensor.__torch_dispatch__:10358 | ProxyTorchDispatchMode.__torch_dispatch__:95430
Dynamo produced 14 graphs covering 481 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=2 for MBartForConditionalGeneration, orig batch_size=4
cuda train MBartForConditionalGeneration       1.009x p=0.00
TIMING: entire_frame_compile:107.53803 backend_compile:100.8491
STATS: call_* op count: 1263 | FakeTensorMode.__torch_dispatch__:253336 | FakeTensor.__torch_dispatch__:23458 | ProxyTorchDispatchMode.__torch_dispatch__:246465
Dynamo produced 1 graphs covering 1263 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for MT5ForConditionalGeneration, orig batch_size=32
WARNING:__main__:Sequence Length not defined for MT5ForConditionalGeneration. Choosing 128 arbitrarily
cuda train MT5ForConditionalGeneration         0.926x p=0.00
TIMING: entire_frame_compile:62.25603 backend_compile:57.27911
STATS: call_* op count: 1173 | FakeTensorMode.__torch_dispatch__:152335 | FakeTensor.__torch_dispatch__:12296 | ProxyTorchDispatchMode.__torch_dispatch__:132682
Dynamo produced 1 graphs covering 1173 ops with 4 graph breaks (3 unique)
If you want to use `MegatronBertForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=4 for MegatronBertForCausalLM, orig batch_size=16
cuda train MegatronBertForCausalLM             1.078x p=0.00
TIMING: entire_frame_compile:84.07869 backend_compile:78.4179
STATS: call_* op count: 721 | FakeTensorMode.__torch_dispatch__:199085 | FakeTensor.__torch_dispatch__:18647 | ProxyTorchDispatchMode.__torch_dispatch__:191581
Dynamo produced 1 graphs covering 721 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=8 for MegatronBertForQuestionAnswering, orig batch_size=16
cuda train MegatronBertForQuestionAnswering    1.097x SAME
TIMING: entire_frame_compile:86.36113 backend_compile:80.83763
STATS: call_* op count: 724 | FakeTensorMode.__torch_dispatch__:198083 | FakeTensor.__torch_dispatch__:18526 | ProxyTorchDispatchMode.__torch_dispatch__:190312
Dynamo produced 1 graphs covering 724 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=64 for MobileBertForMaskedLM, orig batch_size=256
cuda train MobileBertForMaskedLM               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py", line 1067, in forward
    @add_start_docstrings_to_model_forward(MOBILEBERT_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2386, in debug_compiled_function
    return compiled_function(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2151, in forward
    fw_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 477, in nvfuser_execute_partitioned
    gm, is_partitioned = maybe_partition_graph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 422, in maybe_partition_graph
    partitioned_graph = partitioner.fuse_partitions(partitions)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py", line 217, in fuse_partitions
    return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 204, in fuse_by_partitions
    sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule(gm, sorted_nodes, submodule_name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 106, in fuse_as_graphmodule
    assert validate_partition(nodes), "Invalid partition, found dependency cycles"
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 69, in validate_partition
    if dfs_find_cycle(output_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  [Previous line repeated 960 more times]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 61, in dfs_find_cycle
    visited.add(node)
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR
WARNING:__main__:Running smaller batch size=128 for MobileBertForQuestionAnswering, orig batch_size=256
cuda train MobileBertForQuestionAnswering      ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py", line 1363, in forward
    @add_start_docstrings_to_model_forward(MOBILEBERT_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2386, in debug_compiled_function
    return compiled_function(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2151, in forward
    fw_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 477, in nvfuser_execute_partitioned
    gm, is_partitioned = maybe_partition_graph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 422, in maybe_partition_graph
    partitioned_graph = partitioner.fuse_partitions(partitions)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py", line 217, in fuse_partitions
    return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 204, in fuse_by_partitions
    sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule(gm, sorted_nodes, submodule_name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 106, in fuse_as_graphmodule
    assert validate_partition(nodes), "Invalid partition, found dependency cycles"
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 69, in validate_partition
    if dfs_find_cycle(output_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  [Previous line repeated 960 more times]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 61, in dfs_find_cycle
    visited.add(node)
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR
WARNING:__main__:Running smaller batch size=2 for OPTForCausalLM, orig batch_size=4
cuda train OPTForCausalLM                      0.915x p=0.00
TIMING: entire_frame_compile:40.21859 backend_compile:37.27259
STATS: call_* op count: 533 | FakeTensorMode.__torch_dispatch__:96593 | FakeTensor.__torch_dispatch__:9656 | ProxyTorchDispatchMode.__torch_dispatch__:91987
Dynamo produced 14 graphs covering 533 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=8 for PLBartForCausalLM, orig batch_size=16
cuda train PLBartForCausalLM                   0.967x p=0.00
TIMING: entire_frame_compile:24.19062 backend_compile:22.0684
STATS: call_* op count: 254 | FakeTensorMode.__torch_dispatch__:53271 | FakeTensor.__torch_dispatch__:5351 | ProxyTorchDispatchMode.__torch_dispatch__:50443
Dynamo produced 8 graphs covering 254 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=4 for PLBartForConditionalGeneration, orig batch_size=8
cuda train PLBartForConditionalGeneration      0.989x p=0.00
TIMING: entire_frame_compile:57.93094 backend_compile:52.86722
STATS: call_* op count: 658 | FakeTensorMode.__torch_dispatch__:137383 | FakeTensor.__torch_dispatch__:13675 | ProxyTorchDispatchMode.__torch_dispatch__:128777
Dynamo produced 10 graphs covering 658 ops with 14 graph breaks (7 unique)
WARNING:__main__:Running smaller batch size=32 for PegasusForCausalLM, orig batch_size=128
WARNING:__main__:Sequence Length not defined for PegasusForCausalLM. Choosing 128 arbitrarily
cuda train PegasusForCausalLM                  0.963x p=0.00
TIMING: entire_frame_compile:41.52686 backend_compile:38.56229
STATS: call_* op count: 478 | FakeTensorMode.__torch_dispatch__:98465 | FakeTensor.__torch_dispatch__:10188 | ProxyTorchDispatchMode.__torch_dispatch__:94024
Dynamo produced 16 graphs covering 478 ops with 12 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=32 for PegasusForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for PegasusForConditionalGeneration. Choosing 128 arbitrarily
cuda train PegasusForConditionalGeneration     0.994x p=0.00
TIMING: entire_frame_compile:104.5826 backend_compile:101.12173
STATS: call_* op count: 1244 | FakeTensorMode.__torch_dispatch__:245998 | FakeTensor.__torch_dispatch__:9623 | ProxyTorchDispatchMode.__torch_dispatch__:243344
Dynamo produced 7 graphs covering 1244 ops with 14 graph breaks (5 unique)
If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=16 for RobertaForCausalLM, orig batch_size=32
cuda train RobertaForCausalLM                  1.075x p=0.00
TIMING: entire_frame_compile:44.25472 backend_compile:41.35854
STATS: call_* op count: 381 | FakeTensorMode.__torch_dispatch__:102703 | FakeTensor.__torch_dispatch__:9539 | ProxyTorchDispatchMode.__torch_dispatch__:98937
Dynamo produced 1 graphs covering 381 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for RobertaForQuestionAnswering, orig batch_size=32
cuda train RobertaForQuestionAnswering         1.125x p=0.00
TIMING: entire_frame_compile:44.8851 backend_compile:41.97025
STATS: call_* op count: 384 | FakeTensorMode.__torch_dispatch__:101701 | FakeTensor.__torch_dispatch__:9418 | ProxyTorchDispatchMode.__torch_dispatch__:97668
Dynamo produced 1 graphs covering 384 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=256 for Speech2Text2ForCausalLM, orig batch_size=1024
WARNING:__main__:Sequence Length not defined for Speech2Text2ForCausalLM. Choosing 128 arbitrarily
cuda train Speech2Text2ForCausalLM             0.970x p=0.00
TIMING: entire_frame_compile:21.04613 backend_compile:19.32209
STATS: call_* op count: 261 | FakeTensorMode.__torch_dispatch__:49458 | FakeTensor.__torch_dispatch__:5185 | ProxyTorchDispatchMode.__torch_dispatch__:46558
Dynamo produced 10 graphs covering 261 ops with 12 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=4 for T5ForConditionalGeneration, orig batch_size=8
cuda train T5ForConditionalGeneration          0.933x p=0.00
TIMING: entire_frame_compile:42.86862 backend_compile:38.97496
STATS: call_* op count: 798 | FakeTensorMode.__torch_dispatch__:105685 | FakeTensor.__torch_dispatch__:8605 | ProxyTorchDispatchMode.__torch_dispatch__:92317
Dynamo produced 1 graphs covering 798 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for T5Small, orig batch_size=8
cuda train T5Small                             0.931x p=0.00
TIMING: entire_frame_compile:42.56448 backend_compile:39.27119
STATS: call_* op count: 798 | FakeTensorMode.__torch_dispatch__:105685 | FakeTensor.__torch_dispatch__:8605 | ProxyTorchDispatchMode.__torch_dispatch__:92317
Dynamo produced 1 graphs covering 798 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=32 for TrOCRForCausalLM, orig batch_size=64
cuda train TrOCRForCausalLM                    0.980x p=0.00
TIMING: entire_frame_compile:42.47009 backend_compile:39.03195
STATS: call_* op count: 481 | FakeTensorMode.__torch_dispatch__:99683 | FakeTensor.__torch_dispatch__:10304 | ProxyTorchDispatchMode.__torch_dispatch__:95346
Dynamo produced 14 graphs covering 481 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=8 for XGLMForCausalLM, orig batch_size=32
WARNING:__main__:Sequence Length not defined for XGLMForCausalLM. Choosing 128 arbitrarily
cuda train XGLMForCausalLM                     0.293x p=0.00
TIMING: entire_frame_compile:85.33556 backend_compile:79.919
STATS: call_* op count: 998 | FakeTensorMode.__torch_dispatch__:201111 | FakeTensor.__torch_dispatch__:19171 | ProxyTorchDispatchMode.__torch_dispatch__:193658
Dynamo produced 28 graphs covering 998 ops with 12 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=8 for XLNetLMHeadModel, orig batch_size=16
cuda train XLNetLMHeadModel                    ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py", line 1357, in forward
    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2386, in debug_compiled_function
    return compiled_function(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2151, in forward
    fw_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 477, in nvfuser_execute_partitioned
    gm, is_partitioned = maybe_partition_graph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 422, in maybe_partition_graph
    partitioned_graph = partitioner.fuse_partitions(partitions)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py", line 217, in fuse_partitions
    return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 204, in fuse_by_partitions
    sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule(gm, sorted_nodes, submodule_name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 106, in fuse_as_graphmodule
    assert validate_partition(nodes), "Invalid partition, found dependency cycles"
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 69, in validate_partition
    if dfs_find_cycle(output_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  [Previous line repeated 960 more times]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 61, in dfs_find_cycle
    visited.add(node)
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR
WARNING:__main__:Running smaller batch size=16 for YituTechConvBert, orig batch_size=32
cuda train YituTechConvBert                    1.022x p=0.00
TIMING: entire_frame_compile:63.42913 backend_compile:59.32193
STATS: call_* op count: 634 | FakeTensorMode.__torch_dispatch__:143019 | FakeTensor.__torch_dispatch__:13748 | ProxyTorchDispatchMode.__torch_dispatch__:131289
Dynamo produced 4 graphs covering 634 ops with 7 graph breaks (5 unique)
speedup             gmean=1.06x mean=1.063x
abs_latency         gmean=nanx mean=137.312x
compilation_latency mean=79.184 seconds
compression_ratio   mean=0.939x
eager_peak_mem      gmean=nanx mean=14.008x
dynamo_peak_mem     gmean=nanx mean=15.149x
calls_captured      gmean=nanx mean=590.868x
unique_graphs       gmean=nanx mean=6.500x
graph_breaks        gmean=nanx mean=8.026x
unique_graph_breaks gmean=nanx mean=4.789x
