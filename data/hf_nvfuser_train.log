WARNING:__main__:Running smaller batch size=4 for AlbertForMaskedLM, orig batch_size=8
cuda train AlbertForMaskedLM                   1.238x p=0.00
TIMING: entire_frame_compile:40.1866 backend_compile:38.43754
STATS: call_* op count: 439 | FakeTensorMode.__torch_dispatch__:94735 | FakeTensor.__torch_dispatch__:4535 | ProxyTorchDispatchMode.__torch_dispatch__:93155
Dynamo produced 1 graphs covering 439 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for AlbertForQuestionAnswering, orig batch_size=8
cuda train AlbertForQuestionAnswering          1.243x p=0.00
TIMING: entire_frame_compile:41.23942 backend_compile:39.50443
STATS: call_* op count: 439 | FakeTensorMode.__torch_dispatch__:93918 | FakeTensor.__torch_dispatch__:4414 | ProxyTorchDispatchMode.__torch_dispatch__:92121
Dynamo produced 1 graphs covering 439 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for AllenaiLongformerBase, orig batch_size=8
cuda train AllenaiLongformerBase               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 222, in make_nvfuser_fusion
    out = FusionInterpreter(gm).run(*nv_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 136, in run
    self.env[node] = self.run_node(node)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 184, in run_node
    return super().run_node(node)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 177, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 202, in output
    assert isinstance(
AssertionError: output from codegen has to be tensor type

While executing return [view, view_1, sub]
Original traceback:
None

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1848, in forward
    outputs = self.longformer(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1750, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1294, in forward
    is_global_attn = is_index_global_attn.flatten().any().item()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1326, in <graph break in forward>
    layer_outputs = layer_module(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1249, in forward
    self_attn_outputs = self.attention(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1185, in forward
    self_outputs = self.self(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 574, in forward
    attn_scores = self._sliding_chunks_query_key_matmul(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 821, in _sliding_chunks_query_key_matmul
    def _sliding_chunks_query_key_matmul(self, query: torch.Tensor, key: torch.Tensor, window_overlap: int):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2386, in debug_compiled_function
    return compiled_function(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2151, in forward
    fw_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 250, in nvfuser_execute
    fusion, unflatten_spec = make_nvfuser_fusion(gm, *nv_template_args)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 152, in make_nvfuser_fusion
    with FusionDefinition(fusion) as fd:
TypeError: __exit__(): incompatible function arguments. The following argument types are supported:
    1. (self: nvfuser._C.FusionDefinition, arg0: capsule, arg1: capsule, arg2: capsule) -> None

Invoked with: <nvfuser._C.FusionDefinition object at 0x7fa18c733270>, <class 'AssertionError'>, AssertionError('output from codegen has to be tensor type\n\nWhile executing return [view, view_1, sub]\nOriginal traceback:\nNone'), <traceback object at 0x7fa18c733000>
ERROR
WARNING:__main__:Running smaller batch size=4 for BartForCausalLM, orig batch_size=8
cuda train BartForCausalLM                     1.015x p=0.00
TIMING: entire_frame_compile:42.06608 backend_compile:38.95535
STATS: call_* op count: 482 | FakeTensorMode.__torch_dispatch__:99991 | FakeTensor.__torch_dispatch__:10301 | ProxyTorchDispatchMode.__torch_dispatch__:95419
Dynamo produced 14 graphs covering 482 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=2 for BartForConditionalGeneration, orig batch_size=4
cuda train BartForConditionalGeneration        0.433x p=0.00
TIMING: entire_frame_compile:104.82643 backend_compile:98.52467
STATS: call_* op count: 1258 | FakeTensorMode.__torch_dispatch__:251177 | FakeTensor.__torch_dispatch__:23330 | ProxyTorchDispatchMode.__torch_dispatch__:244009
Dynamo produced 1 graphs covering 1258 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for BertForMaskedLM, orig batch_size=32
cuda train BertForMaskedLM                     1.075x p=0.00
TIMING: entire_frame_compile:44.43325 backend_compile:41.49619
STATS: call_* op count: 370 | FakeTensorMode.__torch_dispatch__:102157 | FakeTensor.__torch_dispatch__:9539 | ProxyTorchDispatchMode.__torch_dispatch__:98476
Dynamo produced 1 graphs covering 370 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for BertForQuestionAnswering, orig batch_size=32
cuda train BertForQuestionAnswering            1.131x p=0.00
TIMING: entire_frame_compile:42.75205 backend_compile:39.94575
STATS: call_* op count: 377 | FakeTensorMode.__torch_dispatch__:101486 | FakeTensor.__torch_dispatch__:9418 | ProxyTorchDispatchMode.__torch_dispatch__:97432
Dynamo produced 1 graphs covering 377 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for BlenderbotForCausalLM, orig batch_size=32
cuda train BlenderbotForCausalLM               0.902x p=0.00
TIMING: entire_frame_compile:83.46492 backend_compile:77.17721
STATS: call_* op count: 934 | FakeTensorMode.__torch_dispatch__:194861 | FakeTensor.__torch_dispatch__:20204 | ProxyTorchDispatchMode.__torch_dispatch__:186512
Dynamo produced 26 graphs covering 934 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForCausalLM, orig batch_size=256
cuda train BlenderbotSmallForCausalLM          0.927x p=0.00
TIMING: entire_frame_compile:28.77697 backend_compile:26.38008
STATS: call_* op count: 327 | FakeTensorMode.__torch_dispatch__:68020 | FakeTensor.__torch_dispatch__:7004 | ProxyTorchDispatchMode.__torch_dispatch__:64708
Dynamo produced 10 graphs covering 327 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForConditionalGeneration, orig batch_size=128
cuda train BlenderbotSmallForConditionalGeneration  0.968x p=0.00
TIMING: entire_frame_compile:71.10014 backend_compile:66.64953
STATS: call_* op count: 840 | FakeTensorMode.__torch_dispatch__:168882 | FakeTensor.__torch_dispatch__:15669 | ProxyTorchDispatchMode.__torch_dispatch__:164273
Dynamo produced 1 graphs covering 840 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for CamemBert, orig batch_size=32
cuda train CamemBert                           1.075x p=0.00
TIMING: entire_frame_compile:42.75008 backend_compile:39.94821
STATS: call_* op count: 377 | FakeTensorMode.__torch_dispatch__:102402 | FakeTensor.__torch_dispatch__:9539 | ProxyTorchDispatchMode.__torch_dispatch__:98733
Dynamo produced 1 graphs covering 377 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for DebertaForMaskedLM, orig batch_size=32
cuda train DebertaForMaskedLM                  ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1072, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 975, in forward
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 983, in <graph break in forward>
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 445, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 419, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS111{( ceilDiv(( T8.size[0] * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS112{1}, iS134{( ceilDiv(( ceilDiv(( 32 * 32 ), 4) ), 128) )}, iS135{128}, iS133{4} ] with word size 4 not possible due to invalid stride. Domain: iS135{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=8 for DebertaForQuestionAnswering, orig batch_size=32
cuda train DebertaForQuestionAnswering         ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1401, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 975, in forward
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 983, in <graph break in forward>
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 445, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 419, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS111{( ceilDiv(( T8.size[0] * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS112{1}, iS134{( ceilDiv(( ceilDiv(( 32 * 32 ), 4) ), 128) )}, iS135{128}, iS133{4} ] with word size 4 not possible due to invalid stride. Domain: iS135{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=1 for DebertaV2ForMaskedLM, orig batch_size=8
cuda train DebertaV2ForMaskedLM                ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1171, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1075, in forward
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1083, in <graph break in forward>
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 488, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 452, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS109{( ceilDiv(( 1 * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS110{1}, iS132{( ceilDiv(( ceilDiv(( 32 * 32 ), 2) ), 128) )}, iS133{128}, iS131{2} ] with word size 2 not possible due to invalid stride. Domain: iS133{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=2 for DebertaV2ForQuestionAnswering, orig batch_size=8
cuda train DebertaV2ForQuestionAnswering       ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1501, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1075, in forward
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1083, in <graph break in forward>
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 488, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 452, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS111{( ceilDiv(( T8.size[0] * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS112{1}, iS134{( ceilDiv(( ceilDiv(( 32 * 32 ), 4) ), 128) )}, iS135{128}, iS133{4} ] with word size 4 not possible due to invalid stride. Domain: iS135{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=128 for DistilBertForMaskedLM, orig batch_size=256
WARNING:__main__:Sequence Length not defined for DistilBertForMaskedLM. Choosing 128 arbitrarily
cuda train DistilBertForMaskedLM               0.823x p=0.00
TIMING: entire_frame_compile:22.28668 backend_compile:20.93272
STATS: call_* op count: 206 | FakeTensorMode.__torch_dispatch__:53009 | FakeTensor.__torch_dispatch__:4753 | ProxyTorchDispatchMode.__torch_dispatch__:51631
Dynamo produced 1 graphs covering 206 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=256 for DistilBertForQuestionAnswering, orig batch_size=512
WARNING:__main__:Sequence Length not defined for DistilBertForQuestionAnswering. Choosing 128 arbitrarily
cuda train DistilBertForQuestionAnswering      0.802x p=0.00
TIMING: entire_frame_compile:22.8339 backend_compile:21.44983
STATS: call_* op count: 214 | FakeTensorMode.__torch_dispatch__:52543 | FakeTensor.__torch_dispatch__:4633 | ProxyTorchDispatchMode.__torch_dispatch__:50779
Dynamo produced 1 graphs covering 214 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for DistillGPT2, orig batch_size=32
cuda train DistillGPT2                         1.212x p=0.00
TIMING: entire_frame_compile:18.92886 backend_compile:17.49922
STATS: call_* op count: 330 | FakeTensorMode.__torch_dispatch__:44755 | FakeTensor.__torch_dispatch__:3435 | ProxyTorchDispatchMode.__torch_dispatch__:40393
Dynamo produced 1 graphs covering 330 ops with 4 graph breaks (3 unique)
If you want to use `ElectraForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=32 for ElectraForCausalLM, orig batch_size=64
cuda train ElectraForCausalLM                  1.055x p=0.00
TIMING: entire_frame_compile:45.16652 backend_compile:42.21952
STATS: call_* op count: 375 | FakeTensorMode.__torch_dispatch__:103173 | FakeTensor.__torch_dispatch__:9663 | ProxyTorchDispatchMode.__torch_dispatch__:99251
Dynamo produced 4 graphs covering 375 ops with 7 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=64 for ElectraForQuestionAnswering, orig batch_size=128
cuda train ElectraForQuestionAnswering         1.222x p=0.00
TIMING: entire_frame_compile:44.4047 backend_compile:41.51213
STATS: call_* op count: 378 | FakeTensorMode.__torch_dispatch__:102055 | FakeTensor.__torch_dispatch__:9509 | ProxyTorchDispatchMode.__torch_dispatch__:97955
Dynamo produced 1 graphs covering 378 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for GPT2ForSequenceClassification, orig batch_size=8
cuda train GPT2ForSequenceClassification       1.515x p=0.00
TIMING: entire_frame_compile:35.13115 backend_compile:32.65329
STATS: call_* op count: 644 | FakeTensorMode.__torch_dispatch__:85520 | FakeTensor.__torch_dispatch__:6700 | ProxyTorchDispatchMode.__torch_dispatch__:77094
Dynamo produced 1 graphs covering 644 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for GoogleFnet, orig batch_size=32
cuda train GoogleFnet                          1.506x p=0.00
TIMING: entire_frame_compile:25.13917 backend_compile:23.3722
STATS: call_* op count: 209 | FakeTensorMode.__torch_dispatch__:55588 | FakeTensor.__torch_dispatch__:4545 | ProxyTorchDispatchMode.__torch_dispatch__:56145
Dynamo produced 28 graphs covering 209 ops with 45 graph breaks (7 unique)
WARNING:__main__:Running smaller batch size=16 for LayoutLMForMaskedLM, orig batch_size=32
cuda train LayoutLMForMaskedLM                 1.084x p=0.00
TIMING: entire_frame_compile:44.37102 backend_compile:41.2652
STATS: call_* op count: 396 | FakeTensorMode.__torch_dispatch__:105115 | FakeTensor.__torch_dispatch__:10175 | ProxyTorchDispatchMode.__torch_dispatch__:100469
Dynamo produced 1 graphs covering 396 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for LayoutLMForSequenceClassification, orig batch_size=32
cuda train LayoutLMForSequenceClassification   1.135x p=0.00
TIMING: entire_frame_compile:43.44552 backend_compile:40.38729
STATS: call_* op count: 394 | FakeTensorMode.__torch_dispatch__:103898 | FakeTensor.__torch_dispatch__:10094 | ProxyTorchDispatchMode.__torch_dispatch__:99182
Dynamo produced 1 graphs covering 394 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for M2M100ForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for M2M100ForConditionalGeneration. Choosing 128 arbitrarily
cuda train M2M100ForConditionalGeneration      0.908x p=0.00
TIMING: entire_frame_compile:101.52853 backend_compile:94.54106
STATS: call_* op count: 1200 | FakeTensorMode.__torch_dispatch__:241947 | FakeTensor.__torch_dispatch__:24105 | ProxyTorchDispatchMode.__torch_dispatch__:231705
Dynamo produced 18 graphs covering 1200 ops with 17 graph breaks (7 unique)
WARNING:__main__:Running smaller batch size=4 for MBartForCausalLM, orig batch_size=8
cuda train MBartForCausalLM                    1.014x p=0.00
TIMING: entire_frame_compile:42.12326 backend_compile:38.69096
STATS: call_* op count: 481 | FakeTensorMode.__torch_dispatch__:100013 | FakeTensor.__torch_dispatch__:10358 | ProxyTorchDispatchMode.__torch_dispatch__:95430
Dynamo produced 14 graphs covering 481 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=2 for MBartForConditionalGeneration, orig batch_size=4
cuda train MBartForConditionalGeneration       1.025x p=0.00
TIMING: entire_frame_compile:112.44146 backend_compile:105.65046
STATS: call_* op count: 1263 | FakeTensorMode.__torch_dispatch__:253336 | FakeTensor.__torch_dispatch__:23458 | ProxyTorchDispatchMode.__torch_dispatch__:246465
Dynamo produced 1 graphs covering 1263 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for MT5ForConditionalGeneration, orig batch_size=32
WARNING:__main__:Sequence Length not defined for MT5ForConditionalGeneration. Choosing 128 arbitrarily
cuda train MT5ForConditionalGeneration         0.932x p=0.00
TIMING: entire_frame_compile:60.58555 backend_compile:55.7366
STATS: call_* op count: 1173 | FakeTensorMode.__torch_dispatch__:152335 | FakeTensor.__torch_dispatch__:12296 | ProxyTorchDispatchMode.__torch_dispatch__:132682
Dynamo produced 1 graphs covering 1173 ops with 4 graph breaks (3 unique)
If you want to use `MegatronBertForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=4 for MegatronBertForCausalLM, orig batch_size=16
cuda train MegatronBertForCausalLM             1.079x p=0.00
TIMING: entire_frame_compile:83.62113 backend_compile:78.00258
STATS: call_* op count: 721 | FakeTensorMode.__torch_dispatch__:199085 | FakeTensor.__torch_dispatch__:18647 | ProxyTorchDispatchMode.__torch_dispatch__:191581
Dynamo produced 1 graphs covering 721 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=8 for MegatronBertForQuestionAnswering, orig batch_size=16
cuda train MegatronBertForQuestionAnswering    1.103x SAME
TIMING: entire_frame_compile:82.87895 backend_compile:77.59096
STATS: call_* op count: 724 | FakeTensorMode.__torch_dispatch__:198083 | FakeTensor.__torch_dispatch__:18526 | ProxyTorchDispatchMode.__torch_dispatch__:190312
Dynamo produced 1 graphs covering 724 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=64 for MobileBertForMaskedLM, orig batch_size=256
cuda train MobileBertForMaskedLM               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py", line 1067, in forward
    @add_start_docstrings_to_model_forward(MOBILEBERT_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2386, in debug_compiled_function
    return compiled_function(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2151, in forward
    fw_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 477, in nvfuser_execute_partitioned
    gm, is_partitioned = maybe_partition_graph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 422, in maybe_partition_graph
    partitioned_graph = partitioner.fuse_partitions(partitions)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py", line 217, in fuse_partitions
    return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 204, in fuse_by_partitions
    sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule(gm, sorted_nodes, submodule_name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 106, in fuse_as_graphmodule
    assert validate_partition(nodes), "Invalid partition, found dependency cycles"
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 69, in validate_partition
    if dfs_find_cycle(output_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  [Previous line repeated 960 more times]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 61, in dfs_find_cycle
    visited.add(node)
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR
WARNING:__main__:Running smaller batch size=128 for MobileBertForQuestionAnswering, orig batch_size=256
cuda train MobileBertForQuestionAnswering      ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py", line 1363, in forward
    @add_start_docstrings_to_model_forward(MOBILEBERT_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2386, in debug_compiled_function
    return compiled_function(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2151, in forward
    fw_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 477, in nvfuser_execute_partitioned
    gm, is_partitioned = maybe_partition_graph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 422, in maybe_partition_graph
    partitioned_graph = partitioner.fuse_partitions(partitions)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py", line 217, in fuse_partitions
    return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 204, in fuse_by_partitions
    sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule(gm, sorted_nodes, submodule_name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 106, in fuse_as_graphmodule
    assert validate_partition(nodes), "Invalid partition, found dependency cycles"
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 69, in validate_partition
    if dfs_find_cycle(output_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  [Previous line repeated 960 more times]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 61, in dfs_find_cycle
    visited.add(node)
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR
WARNING:__main__:Running smaller batch size=2 for OPTForCausalLM, orig batch_size=4
cuda train OPTForCausalLM                      0.936x p=0.00
TIMING: entire_frame_compile:39.69114 backend_compile:36.79045
STATS: call_* op count: 533 | FakeTensorMode.__torch_dispatch__:96593 | FakeTensor.__torch_dispatch__:9656 | ProxyTorchDispatchMode.__torch_dispatch__:91987
Dynamo produced 14 graphs covering 533 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=8 for PLBartForCausalLM, orig batch_size=16
cuda train PLBartForCausalLM                   0.968x p=0.00
TIMING: entire_frame_compile:23.92157 backend_compile:21.85455
STATS: call_* op count: 254 | FakeTensorMode.__torch_dispatch__:53271 | FakeTensor.__torch_dispatch__:5351 | ProxyTorchDispatchMode.__torch_dispatch__:50443
Dynamo produced 8 graphs covering 254 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=4 for PLBartForConditionalGeneration, orig batch_size=8
cuda train PLBartForConditionalGeneration      0.991x p=0.00
TIMING: entire_frame_compile:56.93495 backend_compile:51.91589
STATS: call_* op count: 658 | FakeTensorMode.__torch_dispatch__:137383 | FakeTensor.__torch_dispatch__:13675 | ProxyTorchDispatchMode.__torch_dispatch__:128777
Dynamo produced 10 graphs covering 658 ops with 14 graph breaks (7 unique)
WARNING:__main__:Running smaller batch size=32 for PegasusForCausalLM, orig batch_size=128
WARNING:__main__:Sequence Length not defined for PegasusForCausalLM. Choosing 128 arbitrarily
cuda train PegasusForCausalLM                  0.963x p=0.00
TIMING: entire_frame_compile:41.50259 backend_compile:38.53742
STATS: call_* op count: 478 | FakeTensorMode.__torch_dispatch__:98465 | FakeTensor.__torch_dispatch__:10188 | ProxyTorchDispatchMode.__torch_dispatch__:94024
Dynamo produced 16 graphs covering 478 ops with 12 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=32 for PegasusForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for PegasusForConditionalGeneration. Choosing 128 arbitrarily
cuda train PegasusForConditionalGeneration     0.994x p=0.00
TIMING: entire_frame_compile:103.1531 backend_compile:99.70662
STATS: call_* op count: 1244 | FakeTensorMode.__torch_dispatch__:245998 | FakeTensor.__torch_dispatch__:9623 | ProxyTorchDispatchMode.__torch_dispatch__:243344
Dynamo produced 7 graphs covering 1244 ops with 14 graph breaks (5 unique)
If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=16 for RobertaForCausalLM, orig batch_size=32
cuda train RobertaForCausalLM                  1.075x p=0.00
TIMING: entire_frame_compile:43.33633 backend_compile:40.49461
STATS: call_* op count: 381 | FakeTensorMode.__torch_dispatch__:102703 | FakeTensor.__torch_dispatch__:9539 | ProxyTorchDispatchMode.__torch_dispatch__:98937
Dynamo produced 1 graphs covering 381 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=16 for RobertaForQuestionAnswering, orig batch_size=32
cuda train RobertaForQuestionAnswering         1.131x p=0.00
TIMING: entire_frame_compile:42.71657 backend_compile:39.87654
STATS: call_* op count: 384 | FakeTensorMode.__torch_dispatch__:101701 | FakeTensor.__torch_dispatch__:9418 | ProxyTorchDispatchMode.__torch_dispatch__:97668
Dynamo produced 1 graphs covering 384 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=256 for Speech2Text2ForCausalLM, orig batch_size=1024
WARNING:__main__:Sequence Length not defined for Speech2Text2ForCausalLM. Choosing 128 arbitrarily
cuda train Speech2Text2ForCausalLM             0.969x p=0.00
TIMING: entire_frame_compile:20.84185 backend_compile:19.1084
STATS: call_* op count: 261 | FakeTensorMode.__torch_dispatch__:49458 | FakeTensor.__torch_dispatch__:5185 | ProxyTorchDispatchMode.__torch_dispatch__:46558
Dynamo produced 10 graphs covering 261 ops with 12 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=4 for T5ForConditionalGeneration, orig batch_size=8
cuda train T5ForConditionalGeneration          0.937x p=0.00
TIMING: entire_frame_compile:43.31763 backend_compile:38.43039
STATS: call_* op count: 798 | FakeTensorMode.__torch_dispatch__:105685 | FakeTensor.__torch_dispatch__:8605 | ProxyTorchDispatchMode.__torch_dispatch__:92317
Dynamo produced 1 graphs covering 798 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=4 for T5Small, orig batch_size=8
cuda train T5Small                             0.935x p=0.00
TIMING: entire_frame_compile:43.83301 backend_compile:40.43286
STATS: call_* op count: 798 | FakeTensorMode.__torch_dispatch__:105685 | FakeTensor.__torch_dispatch__:8605 | ProxyTorchDispatchMode.__torch_dispatch__:92317
Dynamo produced 1 graphs covering 798 ops with 4 graph breaks (3 unique)
WARNING:__main__:Running smaller batch size=32 for TrOCRForCausalLM, orig batch_size=64
cuda train TrOCRForCausalLM                    0.978x p=0.00
TIMING: entire_frame_compile:44.06877 backend_compile:40.44324
STATS: call_* op count: 481 | FakeTensorMode.__torch_dispatch__:99683 | FakeTensor.__torch_dispatch__:10304 | ProxyTorchDispatchMode.__torch_dispatch__:95346
Dynamo produced 14 graphs covering 481 ops with 11 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=8 for XGLMForCausalLM, orig batch_size=32
WARNING:__main__:Sequence Length not defined for XGLMForCausalLM. Choosing 128 arbitrarily
cuda train XGLMForCausalLM                     0.308x p=0.00
TIMING: entire_frame_compile:85.52441 backend_compile:80.09608
STATS: call_* op count: 998 | FakeTensorMode.__torch_dispatch__:201111 | FakeTensor.__torch_dispatch__:19171 | ProxyTorchDispatchMode.__torch_dispatch__:193658
Dynamo produced 28 graphs covering 998 ops with 12 graph breaks (8 unique)
WARNING:__main__:Running smaller batch size=8 for XLNetLMHeadModel, orig batch_size=16
cuda train XLNetLMHeadModel                    ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <graph break in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py", line 1357, in forward
    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2386, in debug_compiled_function
    return compiled_function(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2151, in forward
    fw_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 477, in nvfuser_execute_partitioned
    gm, is_partitioned = maybe_partition_graph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 422, in maybe_partition_graph
    partitioned_graph = partitioner.fuse_partitions(partitions)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py", line 217, in fuse_partitions
    return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 204, in fuse_by_partitions
    sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule(gm, sorted_nodes, submodule_name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 106, in fuse_as_graphmodule
    assert validate_partition(nodes), "Invalid partition, found dependency cycles"
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 69, in validate_partition
    if dfs_find_cycle(output_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  [Previous line repeated 960 more times]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 61, in dfs_find_cycle
    visited.add(node)
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR
WARNING:__main__:Running smaller batch size=16 for YituTechConvBert, orig batch_size=32
cuda train YituTechConvBert                    1.021x p=0.00
TIMING: entire_frame_compile:61.36392 backend_compile:57.17407
STATS: call_* op count: 634 | FakeTensorMode.__torch_dispatch__:143019 | FakeTensor.__torch_dispatch__:13748 | ProxyTorchDispatchMode.__torch_dispatch__:131289
Dynamo produced 4 graphs covering 634 ops with 7 graph breaks (5 unique)
speedup             gmean=1.06x mean=1.064x
abs_latency         gmean=nanx mean=137.177x
compilation_latency mean=77.930 seconds
compression_ratio   mean=0.939x
eager_peak_mem      gmean=nanx mean=14.008x
dynamo_peak_mem     gmean=nanx mean=15.149x
calls_captured      gmean=nanx mean=590.868x
unique_graphs       gmean=nanx mean=6.500x
graph_breaks        gmean=nanx mean=8.026x
unique_graph_breaks gmean=nanx mean=4.789x
