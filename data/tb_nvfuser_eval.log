cuda eval  BERT_pytorch                        0.828x p=0.00
TIMING: entire_frame_compile:13.57555 backend_compile:12.09463
STATS: call_* op count: 538 | FakeTensorMode.__torch_dispatch__:37463 | FakeTensor.__torch_dispatch__:2049 | ProxyTorchDispatchMode.__torch_dispatch__:27725
Dynamo produced 1 graphs covering 538 ops with 0 graph breaks (0 unique)
cuda eval  Background_Matting                  0.782x p=0.00
TIMING: entire_frame_compile:16.35109 backend_compile:15.34088
STATS: call_* op count: 183 | FakeTensorMode.__torch_dispatch__:42259 | FakeTensor.__torch_dispatch__:1216 | ProxyTorchDispatchMode.__torch_dispatch__:34291
Dynamo produced 1 graphs covering 183 ops with 0 graph breaks (0 unique)
cuda eval  LearningToPaint                     0.862x p=0.00
TIMING: entire_frame_compile:6.6647 backend_compile:6.14657
STATS: call_* op count: 71 | FakeTensor.__torch_dispatch__:374 | FakeTensorMode.__torch_dispatch__:16744 | ProxyTorchDispatchMode.__torch_dispatch__:14133
Dynamo produced 1 graphs covering 71 ops with 0 graph breaks (0 unique)
WARNING:root:Super_SloMo failed to load
Eager model failed to run
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1146, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 363, in forward_pass
    return mod(*inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/Super_SloMo/model_wrapper.py", line 34, in forward
    fCoeff = model.getFlowCoeff(trainFrameIndex, I0.device)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/Super_SloMo/slomo_model.py", line 324, in getFlowCoeff
    C11 = C00 = - (1 - (t[ind])) * (t[ind])
RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 311, in load_model
    self.validate_model(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1148, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

cuda eval  alexnet                             0.991x p=0.00
TIMING: entire_frame_compile:0.93636 backend_compile:0.68189
STATS: call_* op count: 22 | FakeTensor.__torch_dispatch__:110 | FakeTensorMode.__torch_dispatch__:1836 | ProxyTorchDispatchMode.__torch_dispatch__:1350
Dynamo produced 1 graphs covering 22 ops with 0 graph breaks (0 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 249, in load_model
    module = importlib.import_module(f"torchbenchmark.models.{model_name}")
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/attention_is_all_you_need_pytorch/__init__.py", line 12, in <module>
    from torchbenchmark.util.torchtext_legacy.field import Field
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/torchtext_legacy/field.py", line 6, in <module>
    from .data import Dataset
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/torchtext_legacy/data.py", line 13, in <module>
    from torchtext.data.utils import RandomShuffler
  File "/scratch/voz/work/torchtext/torchtext/__init__.py", line 6, in <module>
    from torchtext import _extension  # noqa: F401
  File "/scratch/voz/work/torchtext/torchtext/_extension.py", line 64, in <module>
    _init_extension()
  File "/scratch/voz/work/torchtext/torchtext/_extension.py", line 58, in _init_extension
    _load_lib("libtorchtext")
  File "/scratch/voz/work/torchtext/torchtext/_extension.py", line 50, in _load_lib
    torch.ops.load_library(path)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_ops.py", line 643, in load_library
    ctypes.CDLL(path)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/ctypes/__init__.py", line 374, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: /scratch/voz/work/torchtext/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
ERROR
cuda eval  dcgan                               1.054x p=0.00
TIMING: entire_frame_compile:1.27938 backend_compile:1.0252
STATS: call_* op count: 13 | FakeTensor.__torch_dispatch__:73 | FakeTensorMode.__torch_dispatch__:2600 | ProxyTorchDispatchMode.__torch_dispatch__:2190
Dynamo produced 1 graphs covering 13 ops with 0 graph breaks (0 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 283, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/demucs/__init__.py", line 50, in __init__
    model.to(device)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 202, in _apply
    self._init_flat_weights()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 139, in _init_flat_weights
    self.flatten_parameters()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 169, in flatten_parameters
    not torch.backends.cudnn.is_acceptable(fw.data)):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/backends/cudnn/__init__.py", line 97, in is_acceptable
    if not _init():
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/backends/cudnn/__init__.py", line 52, in _init
    raise RuntimeError(f'{base_error_msg}'
RuntimeError: cuDNN version incompatibility: PyTorch was compiled  against (8, 5, 0) but found runtime version (8, 4, 1). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 5, 0)
ERROR
cuda eval  densenet121                         0.789x p=0.00
TIMING: entire_frame_compile:49.08113 backend_compile:46.94809
STATS: call_* op count: 431 | FakeTensor.__torch_dispatch__:2065 | FakeTensorMode.__torch_dispatch__:115004 | ProxyTorchDispatchMode.__torch_dispatch__:109444
Dynamo produced 1 graphs covering 431 ops with 0 graph breaks (0 unique)
