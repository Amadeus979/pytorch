cuda eval  BERT_pytorch                        1.567x p=0.00
TIMING: entire_frame_compile:7.84381 backend_compile:6.31786
STATS: call_* op count: 538 | FakeTensorMode.__torch_dispatch__:17112 | FakeTensor.__torch_dispatch__:1079 | ProxyTorchDispatchMode.__torch_dispatch__:3512
Dynamo produced 1 graphs covering 538 ops with 0 graph breaks (0 unique)
cuda eval  Background_Matting                  [2023-03-21 09:03:07,827] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.413x p=0.00
TIMING: entire_frame_compile:9.03472 backend_compile:8.02892
STATS: call_* op count: 183 | FakeTensorMode.__torch_dispatch__:17799 | FakeTensor.__torch_dispatch__:1168 | ProxyTorchDispatchMode.__torch_dispatch__:4227
Dynamo produced 1 graphs covering 183 ops with 0 graph breaks (0 unique)
cuda eval  LearningToPaint                     [2023-03-21 09:03:31,153] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.247x p=0.00
TIMING: entire_frame_compile:7.12058 backend_compile:4.75929
STATS: call_* op count: 71 | FakeTensor.__torch_dispatch__:369 | FakeTensorMode.__torch_dispatch__:6713 | ProxyTorchDispatchMode.__torch_dispatch__:1574
Dynamo produced 1 graphs covering 71 ops with 0 graph breaks (0 unique)
WARNING:root:Super_SloMo failed to load
Eager model failed to run
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1146, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 363, in forward_pass
    return mod(*inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/Super_SloMo/model_wrapper.py", line 34, in forward
    fCoeff = model.getFlowCoeff(trainFrameIndex, I0.device)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/Super_SloMo/slomo_model.py", line 324, in getFlowCoeff
    C11 = C00 = - (1 - (t[ind])) * (t[ind])
RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 311, in load_model
    self.validate_model(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1148, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

cuda eval  alexnet                             1.232x p=0.00
TIMING: entire_frame_compile:3.07748 backend_compile:2.82447
STATS: call_* op count: 22 | FakeTensor.__torch_dispatch__:101 | FakeTensorMode.__torch_dispatch__:831 | ProxyTorchDispatchMode.__torch_dispatch__:114
Dynamo produced 1 graphs covering 22 ops with 0 graph breaks (0 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 249, in load_model
    module = importlib.import_module(f"torchbenchmark.models.{model_name}")
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/attention_is_all_you_need_pytorch/__init__.py", line 12, in <module>
    from torchbenchmark.util.torchtext_legacy.field import Field
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/torchtext_legacy/field.py", line 6, in <module>
    from .data import Dataset
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/torchtext_legacy/data.py", line 13, in <module>
    from torchtext.data.utils import RandomShuffler
  File "/scratch/voz/work/torchtext/torchtext/__init__.py", line 6, in <module>
    from torchtext import _extension  # noqa: F401
  File "/scratch/voz/work/torchtext/torchtext/_extension.py", line 64, in <module>
    _init_extension()
  File "/scratch/voz/work/torchtext/torchtext/_extension.py", line 58, in _init_extension
    _load_lib("libtorchtext")
  File "/scratch/voz/work/torchtext/torchtext/_extension.py", line 50, in _load_lib
    torch.ops.load_library(path)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_ops.py", line 643, in load_library
    ctypes.CDLL(path)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/ctypes/__init__.py", line 374, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: /scratch/voz/work/torchtext/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
ERROR
cuda eval  dcgan                               [2023-03-21 09:04:24,994] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.081x p=0.00
TIMING: entire_frame_compile:2.81695 backend_compile:2.56472
STATS: call_* op count: 13 | FakeTensor.__torch_dispatch__:73 | FakeTensorMode.__torch_dispatch__:1047 | ProxyTorchDispatchMode.__torch_dispatch__:246
Dynamo produced 1 graphs covering 13 ops with 0 graph breaks (0 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 283, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/demucs/__init__.py", line 50, in __init__
    model.to(device)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 202, in _apply
    self._init_flat_weights()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 139, in _init_flat_weights
    self.flatten_parameters()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 169, in flatten_parameters
    not torch.backends.cudnn.is_acceptable(fw.data)):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/backends/cudnn/__init__.py", line 97, in is_acceptable
    if not _init():
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/backends/cudnn/__init__.py", line 52, in _init
    raise RuntimeError(f'{base_error_msg}'
RuntimeError: cuDNN version incompatibility: PyTorch was compiled  against (8, 5, 0) but found runtime version (8, 4, 1). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 5, 0)
ERROR
cuda eval  densenet121                         [2023-03-21 09:05:11,707] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.626x p=0.00
TIMING: entire_frame_compile:21.55159 backend_compile:19.44492
STATS: call_* op count: 431 | FakeTensor.__torch_dispatch__:2060 | FakeTensorMode.__torch_dispatch__:36686 | ProxyTorchDispatchMode.__torch_dispatch__:7962
Dynamo produced 1 graphs covering 431 ops with 0 graph breaks (0 unique)
cuda eval  dlrm                                [2023-03-21 09:05:47,283] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
0.948x p=0.00
TIMING: entire_frame_compile:12.77789 backend_compile:10.7624
STATS: call_* op count: 36 | FakeTensor.__torch_dispatch__:273 | FakeTensorMode.__torch_dispatch__:1519 | ProxyTorchDispatchMode.__torch_dispatch__:282
Dynamo produced 1 graphs covering 36 ops with 0 graph breaks (0 unique)
cuda eval  drq                                 3.223x p=0.00
TIMING: entire_frame_compile:3.29665 backend_compile:2.94292
STATS: call_* op count: 29 | FakeTensorMode.__torch_dispatch__:914 | FakeTensor.__torch_dispatch__:110 | ProxyTorchDispatchMode.__torch_dispatch__:152
Dynamo produced 1 graphs covering 29 ops with 0 graph breaks (0 unique)
cuda eval  fastNLP_Bert                        [2023-03-21 09:06:19,534] torch._inductor.utils: [WARNING] DeviceCopy in input program
[2023-03-21 09:06:19,646] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
1.912x p=0.00
TIMING: entire_frame_compile:7.42498 backend_compile:5.65101
STATS: call_* op count: 435 | FakeTensorMode.__torch_dispatch__:17646 | FakeTensor.__torch_dispatch__:1618 | ProxyTorchDispatchMode.__torch_dispatch__:3398
Dynamo produced 5 graphs covering 435 ops with 4 graph breaks (2 unique)
cuda eval  hf_Albert                           5.204x p=0.00
TIMING: entire_frame_compile:9.35281 backend_compile:6.20786
STATS: call_* op count: 436 | FakeTensorMode.__torch_dispatch__:17924 | FakeTensor.__torch_dispatch__:1320 | ProxyTorchDispatchMode.__torch_dispatch__:3120
Dynamo produced 1 graphs covering 436 ops with 0 graph breaks (0 unique)
cuda eval  hf_Bart                             3.021x p=0.00
TIMING: entire_frame_compile:12.97697 backend_compile:9.50014
STATS: call_* op count: 649 | FakeTensorMode.__torch_dispatch__:27505 | FakeTensor.__torch_dispatch__:2304 | ProxyTorchDispatchMode.__torch_dispatch__:4624
Dynamo produced 16 graphs covering 649 ops with 25 graph breaks (7 unique)
cuda eval  hf_Bert                             4.143x p=0.00
TIMING: entire_frame_compile:8.16158 backend_compile:6.36102
STATS: call_* op count: 367 | FakeTensorMode.__torch_dispatch__:19772 | FakeTensor.__torch_dispatch__:1610 | ProxyTorchDispatchMode.__torch_dispatch__:3518
Dynamo produced 1 graphs covering 367 ops with 0 graph breaks (0 unique)
cuda eval  hf_BigBird                          1.663x p=0.00
TIMING: entire_frame_compile:35.75144 backend_compile:27.35364
STATS: call_* op count: 2857 | FakeTensorMode.__torch_dispatch__:91181 | FakeTensor.__torch_dispatch__:2960 | ProxyTorchDispatchMode.__torch_dispatch__:18744
Dynamo produced 64 graphs covering 2857 ops with 56 graph breaks (5 unique)
cuda eval  hf_DistilBert                       3.884x p=0.00
TIMING: entire_frame_compile:5.21872 backend_compile:4.33816
STATS: call_* op count: 203 | FakeTensorMode.__torch_dispatch__:10096 | FakeTensor.__torch_dispatch__:632 | ProxyTorchDispatchMode.__torch_dispatch__:1923
Dynamo produced 1 graphs covering 203 ops with 0 graph breaks (0 unique)
cuda eval  hf_GPT2                             2.292x p=0.00
TIMING: entire_frame_compile:8.10115 backend_compile:6.30438
STATS: call_* op count: 635 | FakeTensorMode.__torch_dispatch__:17467 | FakeTensor.__torch_dispatch__:704 | ProxyTorchDispatchMode.__torch_dispatch__:3366
Dynamo produced 1 graphs covering 635 ops with 0 graph breaks (0 unique)
cuda eval  hf_Longformer                       [2023-03-21 09:09:48,930] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
[2023-03-21 09:09:48,964] torch._inductor.graph: [ERROR] Error from lowering
Traceback (most recent call last):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/graph.py", line 333, in call_function
    out = lowerings[target](*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 226, in wrapped
    validate_ir(out)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/ir.py", line 105, in validate_ir
    _check_tensorbox(node_or_nodes)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/ir.py", line 90, in _check_tensorbox
    assert isinstance(
AssertionError: Found <class 'torch._inductor.ir.DynamicScalar'>, which is not a supported top level IR node. See [Note: Inductor IR]
ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/graph.py", line 333, in call_function
    out = lowerings[target](*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 226, in wrapped
    validate_ir(out)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/ir.py", line 105, in validate_ir
    _check_tensorbox(node_or_nodes)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/ir.py", line 90, in _check_tensorbox
    assert isinstance(
AssertionError: Found <class 'torch._inductor.ir.DynamicScalar'>, which is not a supported top level IR node. See [Note: Inductor IR]

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 670, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py", line 1055, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/backends/inductor.py", line 9, in inductor
    return compile_fx(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 455, in compile_fx
    return aot_autograd(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 48, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2805, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2498, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1713, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1326, in aot_dispatch_base
    compiled_fw = aot_config.fw_compiler(fw_module, flat_args_with_views_handled)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 430, in fw_compiler
    return inner_compile(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py", line 595, in debug_wrapper
    compiled_fn = compiler_fn(gm, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/debug.py", line 239, in inner
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 176, in compile_fx_inner
    graph.run(*example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/graph.py", line 194, in run
    return super().run(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 136, in run
    self.env[node] = self.run_node(node)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/graph.py", line 407, in run_node
    result = super().run_node(n)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 177, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/graph.py", line 337, in call_function
    raise LoweringException(e, target, args, kwargs) from e
torch._inductor.exc.LoweringException: AssertionError: Found <class 'torch._inductor.ir.DynamicScalar'>, which is not a supported top level IR node. See [Note: Inductor IR]
  target: aten._local_scalar_dense.default
  args[0]: TensorBox(StorageBox(
    Pointwise(
      'cpu',
      torch.int64,
      tmp0 = constant(4096, torch.int64)
      tmp1 = constant(512, torch.int64)
      tmp2 = truncdiv(tmp0, tmp1)
      return tmp2
      ,
      ranges=(),
      origins={div}
    )
  ))

While executing %_local_scalar_dense : [#users=0] = call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%div,), kwargs = {})
Original traceback:
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 769, in _chunk
    hidden_states = hidden_states.view(
 |   File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 839, in <graph break in _sliding_chunks_query_key_matmul>
    query = self._chunk(query, window_overlap, getattr(self.config, "onnx_export", False))


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 363, in forward_pass
    return mod(*inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1848, in forward
    outputs = self.longformer(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1750, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1294, in forward
    is_global_attn = is_index_global_attn.flatten().any().item()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1326, in <graph break in forward>
    layer_outputs = layer_module(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1249, in forward
    self_attn_outputs = self.attention(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1185, in forward
    self_outputs = self.self(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 574, in forward
    attn_scores = self._sliding_chunks_query_key_matmul(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 839, in _sliding_chunks_query_key_matmul
    query = self._chunk(query, window_overlap, getattr(self.config, "onnx_export", False))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 337, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 404, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 104, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 262, in _convert_frame_assert
    return _compile(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 324, in _compile
    out_code = transform_code_object(code, transform)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 445, in transform_code_object
    transformations(instructions, code_options)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 311, in transform
    tracer.run()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1726, in run
    super().run()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 576, in run
    and self.step()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 540, in step
    getattr(self, inst.opname)(inst)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 372, in wrapper
    self.output.compile_subgraph(self, reason=reason)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 541, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 588, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 675, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e) from e
torch._dynamo.exc.BackendCompilerFailed: inductor raised LoweringException: AssertionError: Found <class 'torch._inductor.ir.DynamicScalar'>, which is not a supported top level IR node. See [Note: Inductor IR]
  target: aten._local_scalar_dense.default
  args[0]: TensorBox(StorageBox(
    Pointwise(
      'cpu',
      torch.int64,
      tmp0 = constant(4096, torch.int64)
      tmp1 = constant(512, torch.int64)
      tmp2 = truncdiv(tmp0, tmp1)
      return tmp2
      ,
      ranges=(),
      origins={div}
    )
  ))

While executing %_local_scalar_dense : [#users=0] = call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%div,), kwargs = {})
Original traceback:
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 769, in _chunk
    hidden_states = hidden_states.view(
 |   File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 839, in <graph break in _sliding_chunks_query_key_matmul>
    query = self._chunk(query, window_overlap, getattr(self.config, "onnx_export", False))


Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

ERROR
cuda eval  hf_Reformer                         [2023-03-21 09:10:04,057] torch._inductor.utils: [WARNING] skipping cudagraphs due to complex input striding
[2023-03-21 09:10:06,372] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
[2023-03-21 09:10:07,995] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-03-21 09:10:10,558] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
[2023-03-21 09:10:12,901] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
[2023-03-21 09:10:13,817] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-03-21 09:10:14,373] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
[2023-03-21 09:10:15,503] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
[2023-03-21 09:10:16,439] torch._inductor.utils: [WARNING] using triton random, expect difference from eager
[2023-03-21 09:10:17,000] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
1.673x p=0.00
TIMING: entire_frame_compile:12.37431 backend_compile:9.81297
STATS: call_* op count: 494 | FakeTensorMode.__torch_dispatch__:14779 | FakeTensor.__torch_dispatch__:1090 | ProxyTorchDispatchMode.__torch_dispatch__:3198
Dynamo produced 44 graphs covering 494 ops with 26 graph breaks (3 unique)
cuda eval  hf_T5                               3.159x p=0.00
TIMING: entire_frame_compile:11.46223 backend_compile:8.5782
STATS: call_* op count: 977 | FakeTensorMode.__torch_dispatch__:24399 | FakeTensor.__torch_dispatch__:1784 | ProxyTorchDispatchMode.__torch_dispatch__:6293
Dynamo produced 1 graphs covering 977 ops with 0 graph breaks (0 unique)
cuda eval  maml                                0.833x p=0.00
TIMING: entire_frame_compile:12.17264 backend_compile:9.34121
STATS: call_* op count: 137 | FakeTensorMode.__torch_dispatch__:11206 | ProxyTorchDispatchMode.__torch_dispatch__:3988 | FakeTensor.__torch_dispatch__:816
Dynamo produced 12 graphs covering 137 ops with 5 graph breaks (3 unique)
cuda eval  maml_omniglot                       [2023-03-21 09:11:35,522] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.119x p=0.00
TIMING: entire_frame_compile:3.04171 backend_compile:2.78425
STATS: call_* op count: 14 | FakeTensor.__torch_dispatch__:75 | FakeTensorMode.__torch_dispatch__:1157 | ProxyTorchDispatchMode.__torch_dispatch__:258
Dynamo produced 1 graphs covering 14 ops with 0 graph breaks (0 unique)
cuda eval  mnasnet1_0                          [2023-03-21 09:11:55,646] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.580x p=0.00
TIMING: entire_frame_compile:8.28175 backend_compile:7.25956
STATS: call_* op count: 152 | FakeTensor.__torch_dispatch__:896 | FakeTensorMode.__torch_dispatch__:15655 | ProxyTorchDispatchMode.__torch_dispatch__:3359
Dynamo produced 1 graphs covering 152 ops with 0 graph breaks (0 unique)
cuda eval  mobilenet_v2                        [2023-03-21 09:12:18,028] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.642x p=0.00
TIMING: entire_frame_compile:8.49685 backend_compile:7.55458
STATS: call_* op count: 153 | FakeTensor.__torch_dispatch__:931 | FakeTensorMode.__torch_dispatch__:16033 | ProxyTorchDispatchMode.__torch_dispatch__:3821
Dynamo produced 1 graphs covering 153 ops with 0 graph breaks (0 unique)
WARNING:root:mobilenet_v2_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 283, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/mobilenet_v2_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

cuda eval  mobilenet_v3_large                  [2023-03-21 09:12:48,473] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.446x p=0.00
TIMING: entire_frame_compile:8.99384 backend_compile:8.00276
STATS: call_* op count: 187 | FakeTensor.__torch_dispatch__:1035 | FakeTensorMode.__torch_dispatch__:16680 | ProxyTorchDispatchMode.__torch_dispatch__:3827
Dynamo produced 1 graphs covering 187 ops with 0 graph breaks (0 unique)
cuda eval  moco                                [2023-03-21 09:13:09,703] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
[2023-03-21 09:13:11,395] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
[2023-03-21 09:13:12,182] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
[2023-03-21 09:13:13,075] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
[2023-03-21 09:13:49,308] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)
   function: '<graph break in _momentum_update_key_encoder>' (/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py:50)
   reasons:  ___tuple_iterator_len(___stack0) == 160
to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.
ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 670, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/backends/distributed.py", line 203, in compile_fn
    return self.backend_compile_fn(gm, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py", line 1055, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/backends/inductor.py", line 9, in inductor
    return compile_fx(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 455, in compile_fx
    return aot_autograd(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 48, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2805, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2498, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1674, in aot_wrapper_dedupe
    fw_metadata, _out = run_functionalized_fw_and_collect_metadata(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 606, in inner
    flat_f_outs = f(*flat_f_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2776, in functional_call
    out = Interpreter(mod).run(*args[params_len:], **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 136, in run
    self.env[node] = self.run_node(node)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 177, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 249, in call_function
    return target(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1436, in wrapper
    return func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2433, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/utils/_stats.py", line 20, in wrapper
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py", line 987, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py", line 1066, in dispatch
    args, kwargs = self.validate_and_convert_non_fake_tensors(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py", line 1220, in validate_and_convert_non_fake_tensors
    return tree_map_only(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/utils/_pytree.py", line 266, in tree_map_only
    return tree_map(map_only(ty)(fn), pytree)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/utils/_pytree.py", line 196, in tree_map
    return tree_unflatten([fn(i) for i in flat_args], spec)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/utils/_pytree.py", line 196, in <listcomp>
    return tree_unflatten([fn(i) for i in flat_args], spec)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/utils/_pytree.py", line 247, in inner
    return f(x)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py", line 1212, in validate
    raise Exception(
Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in c10d.allgather_.default(*([[_to_functional_tensor(FakeTensor(FakeTensor(..., device='meta', size=(32, 3, 224, 224)), cuda:0),
       device='cuda:0')]], [FakeTensor(FakeTensor(..., device='meta', size=(32, 3, 224, 224)), cuda:0)], <torch.ScriptObject object at 0x7f8476fb80f0>, -1), **{}) 

While executing %all_gather : [#users=0] = call_function[target=torch.distributed.distributed_c10d.all_gather](args = ([%ones_like], %tensor), kwargs = {async_op: False})
Original traceback:
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 172, in concat_all_gather
    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 363, in forward_pass
    return mod(*inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 130, in forward
    self._momentum_update_key_encoder()  # update the key encoder
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 133, in <graph break in forward>
    im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 76, in _batch_shuffle_ddp
    x_gather = concat_all_gather(x)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 334, in catch_errors
    return hijacked_callback(frame, cache_size, hooks)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 404, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 104, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 262, in _convert_frame_assert
    return _compile(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 324, in _compile
    out_code = transform_code_object(code, transform)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 445, in transform_code_object
    transformations(instructions, code_options)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 311, in transform
    tracer.run()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1726, in run
    super().run()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 576, in run
    and self.step()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 540, in step
    getattr(self, inst.opname)(inst)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1792, in RETURN_VALUE
    self.output.compile_subgraph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 517, in compile_subgraph
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 588, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 675, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e) from e
torch._dynamo.exc.BackendCompilerFailed: compile_fn raised Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in c10d.allgather_.default(*([[_to_functional_tensor(FakeTensor(FakeTensor(..., device='meta', size=(32, 3, 224, 224)), cuda:0),
       device='cuda:0')]], [FakeTensor(FakeTensor(..., device='meta', size=(32, 3, 224, 224)), cuda:0)], <torch.ScriptObject object at 0x7f8476fb80f0>, -1), **{}) 

While executing %all_gather : [#users=0] = call_function[target=torch.distributed.distributed_c10d.all_gather](args = ([%ones_like], %tensor), kwargs = {async_op: False})
Original traceback:
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 172, in concat_all_gather
    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)


Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

ERROR
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 249, in load_model
    module = importlib.import_module(f"torchbenchmark.models.{model_name}")
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/nvidia_deeprecommender/__init__.py", line 12, in <module>
    from torchbenchmark.models.attention_is_all_you_need_pytorch.train import train
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/attention_is_all_you_need_pytorch/__init__.py", line 12, in <module>
    from torchbenchmark.util.torchtext_legacy.field import Field
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/torchtext_legacy/field.py", line 6, in <module>
    from .data import Dataset
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/torchtext_legacy/data.py", line 13, in <module>
    from torchtext.data.utils import RandomShuffler
  File "/scratch/voz/work/torchtext/torchtext/__init__.py", line 6, in <module>
    from torchtext import _extension  # noqa: F401
  File "/scratch/voz/work/torchtext/torchtext/_extension.py", line 64, in <module>
    _init_extension()
  File "/scratch/voz/work/torchtext/torchtext/_extension.py", line 58, in _init_extension
    _load_lib("libtorchtext")
  File "/scratch/voz/work/torchtext/torchtext/_extension.py", line 50, in _load_lib
    torch.ops.load_library(path)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_ops.py", line 643, in load_library
    ctypes.CDLL(path)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/ctypes/__init__.py", line 374, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: /scratch/voz/work/torchtext/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
ERROR
cuda eval  opacus_cifar10                      3.079x p=0.00
TIMING: entire_frame_compile:8.41514 backend_compile:5.03515
STATS: call_* op count: 69 | FakeTensor.__torch_dispatch__:10452 | FakeTensorMode.__torch_dispatch__:13011 | ProxyTorchDispatchMode.__torch_dispatch__:874
Dynamo produced 1 graphs covering 69 ops with 0 graph breaks (0 unique)
cuda eval  pyhpc_equation_of_state             1.399x p=0.00
TIMING: entire_frame_compile:5.18065 backend_compile:4.68548
STATS: call_* op count: 366 | FakeTensorMode.__torch_dispatch__:4116 | ProxyTorchDispatchMode.__torch_dispatch__:1122
Dynamo produced 1 graphs covering 366 ops with 0 graph breaks (0 unique)
cuda eval  pyhpc_isoneutral_mixing             [2023-03-21 09:14:58,992] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
0.855x p=0.00
TIMING: entire_frame_compile:13.32167 backend_compile:12.20276
STATS: call_* op count: 744 | FakeTensorMode.__torch_dispatch__:25105 | FakeTensor.__torch_dispatch__:4 | ProxyTorchDispatchMode.__torch_dispatch__:5398
Dynamo produced 1 graphs covering 744 ops with 0 graph breaks (0 unique)
cuda eval  pyhpc_turbulent_kinetic_energy      [2023-03-21 09:15:24,403] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
2.812x p=0.00
TIMING: entire_frame_compile:13.46152 backend_compile:12.29665
STATS: call_* op count: 992 | FakeTensorMode.__torch_dispatch__:28226 | FakeTensor.__torch_dispatch__:14 | ProxyTorchDispatchMode.__torch_dispatch__:7004
Dynamo produced 1 graphs covering 992 ops with 0 graph breaks (0 unique)
cuda eval  pytorch_CycleGAN_and_pix2pix        1.403x p=0.00
TIMING: entire_frame_compile:5.32018 backend_compile:4.82913
STATS: call_* op count: 91 | FakeTensorMode.__torch_dispatch__:5985 | FakeTensor.__torch_dispatch__:392 | ProxyTorchDispatchMode.__torch_dispatch__:1339
Dynamo produced 1 graphs covering 91 ops with 0 graph breaks (0 unique)
cuda eval  pytorch_stargan                     [2023-03-21 09:16:00,529] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.198x p=0.00
TIMING: entire_frame_compile:5.49846 backend_compile:4.98257
STATS: call_* op count: 56 | FakeTensorMode.__torch_dispatch__:8065 | FakeTensor.__torch_dispatch__:574 | ProxyTorchDispatchMode.__torch_dispatch__:2028
Dynamo produced 1 graphs covering 56 ops with 0 graph breaks (0 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 249, in load_model
    module = importlib.import_module(f"torchbenchmark.models.{model_name}")
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/pytorch_struct/__init__.py", line 6, in <module>
    import pytest
ModuleNotFoundError: No module named 'pytest'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 251, in load_model
    module = importlib.import_module(f"torchbenchmark.models.fb.{model_name}")
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'torchbenchmark.models.fb'
ERROR
cuda eval  pytorch_unet                        [2023-03-21 09:16:28,417] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.556x p=0.00
TIMING: entire_frame_compile:5.40351 backend_compile:4.82869
STATS: call_* op count: 71 | FakeTensor.__torch_dispatch__:455 | FakeTensorMode.__torch_dispatch__:8528 | ProxyTorchDispatchMode.__torch_dispatch__:2045
Dynamo produced 1 graphs covering 71 ops with 0 graph breaks (0 unique)
cuda eval  resnet18                            [2023-03-21 09:16:47,398] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.546x p=0.00
TIMING: entire_frame_compile:5.10903 backend_compile:4.58771
STATS: call_* op count: 69 | FakeTensor.__torch_dispatch__:352 | FakeTensorMode.__torch_dispatch__:6143 | ProxyTorchDispatchMode.__torch_dispatch__:1314
Dynamo produced 1 graphs covering 69 ops with 0 graph breaks (0 unique)
cuda eval  resnet50                            [2023-03-21 09:17:08,516] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.459x p=0.00
TIMING: entire_frame_compile:8.47429 backend_compile:7.5164
STATS: call_* op count: 175 | FakeTensor.__torch_dispatch__:913 | FakeTensorMode.__torch_dispatch__:15946 | ProxyTorchDispatchMode.__torch_dispatch__:3433
Dynamo produced 1 graphs covering 175 ops with 0 graph breaks (0 unique)
WARNING:root:resnet50_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 283, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/resnet50_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

cuda eval  resnext50_32x4d                     [2023-03-21 09:17:40,199] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.639x p=0.00
TIMING: entire_frame_compile:8.71934 backend_compile:7.7621
STATS: call_* op count: 175 | FakeTensor.__torch_dispatch__:913 | FakeTensorMode.__torch_dispatch__:15946 | ProxyTorchDispatchMode.__torch_dispatch__:3433
Dynamo produced 1 graphs covering 175 ops with 0 graph breaks (0 unique)
cuda eval  shufflenet_v2_x1_0                  [2023-03-21 09:18:02,839] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.577x p=0.00
TIMING: entire_frame_compile:9.36332 backend_compile:8.2763
STATS: call_* op count: 271 | FakeTensor.__torch_dispatch__:964 | FakeTensorMode.__torch_dispatch__:18372 | ProxyTorchDispatchMode.__torch_dispatch__:3924
Dynamo produced 1 graphs covering 271 ops with 0 graph breaks (0 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 283, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/soft_actor_critic/__init__.py", line 131, in __init__
    self.train_env = load_gym(self.args.env_id, self.args.seed)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/soft_actor_critic/envs.py", line 237, in load_gym
    env.seed(seed)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/gym/core.py", line 241, in __getattr__
    return getattr(self.env, name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/gym/core.py", line 241, in __getattr__
    return getattr(self.env, name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/gym/core.py", line 241, in __getattr__
    return getattr(self.env, name)
  [Previous line repeated 1 more time]
AttributeError: 'PendulumEnv' object has no attribute 'seed'
ERROR
cuda eval  speech_transformer                  [2023-03-21 09:18:33,200] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
[2023-03-21 09:18:42,093] torch._inductor.utils: [WARNING] skipping cudagraphs due to multiple devices
1.329x p=0.00
TIMING: entire_frame_compile:13.97074 backend_compile:12.20978
STATS: call_* op count: 749 | FakeTensorMode.__torch_dispatch__:23663 | ProxyTorchDispatchMode.__torch_dispatch__:4925 | FakeTensor.__torch_dispatch__:1674
Dynamo produced 9 graphs covering 749 ops with 9 graph breaks (2 unique)
cuda eval  squeezenet1_1                       2.443x p=0.00
TIMING: entire_frame_compile:3.70842 backend_compile:3.37088
STATS: call_* op count: 66 | FakeTensor.__torch_dispatch__:338 | FakeTensorMode.__torch_dispatch__:1876 | ProxyTorchDispatchMode.__torch_dispatch__:308
Dynamo produced 1 graphs covering 66 ops with 0 graph breaks (0 unique)
ERROR
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 249, in load_model
    module = importlib.import_module(f"torchbenchmark.models.{model_name}")
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/timm_efficientdet/__init__.py", line 12, in <module>
    from effdet import create_model, create_loader
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/effdet/__init__.py", line 2, in <module>
    from .bench import DetBenchPredict, DetBenchTrain, unwrap_bench
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/effdet/bench.py", line 70, in <module>
    def _batch_detection(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
RuntimeError: 
object has no attribute nms:
  File "/scratch/voz/work/torchvision/torchvision/ops/boxes.py", line 41
        _log_api_usage_once(nms)
    _assert_has_ops()
    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)
           ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
'nms' is being compiled since it was called from '_batched_nms_vanilla'
  File "/scratch/voz/work/torchvision/torchvision/ops/boxes.py", line 109
    for class_id in torch.unique(idxs):
        curr_indices = torch.where(idxs == class_id)[0]
        curr_keep_indices = nms(boxes[curr_indices], scores[curr_indices], iou_threshold)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        keep_mask[curr_indices[curr_keep_indices]] = True
    keep_indices = torch.where(keep_mask)[0]
'_batched_nms_vanilla' is being compiled since it was called from 'batched_nms'
  File "/scratch/voz/work/torchvision/torchvision/ops/boxes.py", line 73
    # https://github.com/pytorch/vision/issues/1311#issuecomment-781329339
    if boxes.numel() > (4000 if boxes.device.type == "cpu" else 20000) and not torchvision._is_tracing():
        return _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)
               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    else:
        return _batched_nms_coordinate_trick(boxes, scores, idxs, iou_threshold)
'batched_nms' is being compiled since it was called from 'generate_detections'
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/effdet/anchors.py", line 140
        scores[top_detection_idx] = soft_scores
    else:
        top_detection_idx = batched_nms(boxes, scores, classes, iou_threshold=0.5)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE

    # keep only top max_det_per_image scoring predictions
'generate_detections' is being compiled since it was called from '_batch_detection'
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/effdet/bench.py", line 82
        img_scale_i = None if img_scale is None else img_scale[i]
        img_size_i = None if img_size is None else img_size[i]
        detections = generate_detections(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            class_out[i], box_out[i], anchor_boxes, indices[i], classes[i],
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            img_scale_i, img_size_i, max_det_per_image=max_det_per_image, soft_nms=soft_nms)
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        batch_detections.append(detections)
    return torch.stack(batch_detections, dim=0)

ERROR
cuda eval  timm_efficientnet                   [2023-03-21 09:19:49,840] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.237x p=0.00
TIMING: entire_frame_compile:10.42668 backend_compile:8.83118
STATS: call_* op count: 313 | FakeTensor.__torch_dispatch__:1114 | FakeTensorMode.__torch_dispatch__:18194 | ProxyTorchDispatchMode.__torch_dispatch__:3914
Dynamo produced 1 graphs covering 313 ops with 0 graph breaks (0 unique)
cuda eval  timm_nfnet                          1.953x p=0.00
TIMING: entire_frame_compile:12.42436 backend_compile:10.11856
STATS: call_* op count: 615 | FakeTensorMode.__torch_dispatch__:21128 | FakeTensor.__torch_dispatch__:1113 | ProxyTorchDispatchMode.__torch_dispatch__:4948
Dynamo produced 1 graphs covering 615 ops with 0 graph breaks (0 unique)
cuda eval  timm_regnet                         [2023-03-21 09:20:51,215] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.872x p=0.00
TIMING: entire_frame_compile:15.71305 backend_compile:11.74574
STATS: call_* op count: 642 | FakeTensor.__torch_dispatch__:1832 | FakeTensorMode.__torch_dispatch__:29482 | ProxyTorchDispatchMode.__torch_dispatch__:6244
Dynamo produced 1 graphs covering 642 ops with 0 graph breaks (0 unique)
cuda eval  timm_resnest                        [2023-03-21 09:21:14,168] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.636x p=0.00
TIMING: entire_frame_compile:7.55767 backend_compile:6.84691
STATS: call_* op count: 148 | FakeTensor.__torch_dispatch__:471 | FakeTensorMode.__torch_dispatch__:8276 | ProxyTorchDispatchMode.__torch_dispatch__:1791
Dynamo produced 1 graphs covering 148 ops with 0 graph breaks (0 unique)
cuda eval  timm_vision_transformer             2.815x p=0.00
TIMING: entire_frame_compile:8.36395 backend_compile:6.90368
STATS: call_* op count: 337 | FakeTensor.__torch_dispatch__:819 | FakeTensorMode.__torch_dispatch__:11983 | ProxyTorchDispatchMode.__torch_dispatch__:2218
Dynamo produced 1 graphs covering 337 ops with 0 graph breaks (0 unique)
cuda eval  timm_vovnet                         [2023-03-21 09:22:08,671] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.301x p=0.00
TIMING: entire_frame_compile:8.59618 backend_compile:6.73404
STATS: call_* op count: 169 | FakeTensor.__torch_dispatch__:519 | FakeTensorMode.__torch_dispatch__:11452 | ProxyTorchDispatchMode.__torch_dispatch__:2473
Dynamo produced 1 graphs covering 169 ops with 0 graph breaks (0 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 283, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tts_angular/__init__.py", line 20, in __init__
    self.model = TTSModel(device=self.device, batch_size=self.batch_size)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tts_angular/angular_tts_main.py", line 244, in __init__
    self.model = self.model.cuda()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 905, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 202, in _apply
    self._init_flat_weights()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 139, in _init_flat_weights
    self.flatten_parameters()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 169, in flatten_parameters
    not torch.backends.cudnn.is_acceptable(fw.data)):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/backends/cudnn/__init__.py", line 97, in is_acceptable
    if not _init():
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/backends/cudnn/__init__.py", line 52, in _init
    raise RuntimeError(f'{base_error_msg}'
RuntimeError: cuDNN version incompatibility: PyTorch was compiled  against (8, 5, 0) but found runtime version (8, 4, 1). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 5, 0)
ERROR
cuda eval  vgg16                               1.270x p=0.00
TIMING: entire_frame_compile:3.10493 backend_compile:2.81486
STATS: call_* op count: 40 | FakeTensor.__torch_dispatch__:205 | FakeTensorMode.__torch_dispatch__:1385 | ProxyTorchDispatchMode.__torch_dispatch__:202
Dynamo produced 1 graphs covering 40 ops with 0 graph breaks (0 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 386, in <module>
    torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 382, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2269, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 249, in load_model
    module = importlib.import_module(f"torchbenchmark.models.{model_name}")
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/vision_maskrcnn/__init__.py", line 17, in <module>
    from .coco_utils import ConvertCocoPolysToMask
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/vision_maskrcnn/coco_utils.py", line 2, in <module>
    from pycocotools import mask as coco_mask
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/pycocotools-2.0.6-py3.10-linux-x86_64.egg/pycocotools/mask.py", line 3, in <module>
    import pycocotools._mask as _mask
  File "pycocotools/_mask.pyx", line 1, in init pycocotools._mask
ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject
ERROR
cuda eval  yolov3                              [2023-03-21 09:23:14,686] torch._inductor.utils: [WARNING] skipping cudagraphs due to input mutation
1.182x p=0.00
TIMING: entire_frame_compile:16.22517 backend_compile:13.26015
STATS: call_* op count: 308 | FakeTensor.__torch_dispatch__:1364 | FakeTensorMode.__torch_dispatch__:24903 | ProxyTorchDispatchMode.__torch_dispatch__:6029
Dynamo produced 1 graphs covering 308 ops with 0 graph breaks (0 unique)
speedup             gmean=1.55x mean=1.719x
abs_latency         gmean=nanx mean=6.886x
compilation_latency mean=10.563 seconds
compression_ratio   mean=0.871x
eager_peak_mem      gmean=nanx mean=1.118x
dynamo_peak_mem     gmean=nanx mean=1.253x
calls_captured      gmean=nanx mean=360.116x
unique_graphs       gmean=nanx mean=4.349x
graph_breaks        gmean=nanx mean=3.767x
unique_graph_breaks gmean=nanx mean=1.372x
