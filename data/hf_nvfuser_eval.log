WARNING:__main__:Running smaller batch size=4 for AlbertForMaskedLM, orig batch_size=8
cuda eval  AlbertForMaskedLM                   1.268x p=0.00
TIMING: entire_frame_compile:14.87456 backend_compile:13.40212
STATS: call_* op count: 439 | FakeTensorMode.__torch_dispatch__:38227 | FakeTensor.__torch_dispatch__:2323 | ProxyTorchDispatchMode.__torch_dispatch__:31663
Dynamo produced 1 graphs covering 439 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=4 for AlbertForQuestionAnswering, orig batch_size=8
cuda eval  AlbertForQuestionAnswering          1.270x p=0.00
TIMING: entire_frame_compile:15.36826 backend_compile:13.55497
STATS: call_* op count: 439 | FakeTensorMode.__torch_dispatch__:38328 | FakeTensor.__torch_dispatch__:2296 | ProxyTorchDispatchMode.__torch_dispatch__:31656
Dynamo produced 1 graphs covering 439 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=4 for AllenaiLongformerBase, orig batch_size=8
cuda eval  AllenaiLongformerBase               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 222, in make_nvfuser_fusion
    out = FusionInterpreter(gm).run(*nv_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 136, in run
    self.env[node] = self.run_node(node)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 184, in run_node
    return super().run_node(node)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/interpreter.py", line 177, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 202, in output
    assert isinstance(
AssertionError: output from codegen has to be tensor type

While executing return (view, view_1, sub)
Original traceback:
None

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 484, in forward_pass
    return mod(**inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1848, in forward
    outputs = self.longformer(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1750, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1294, in forward
    is_global_attn = is_index_global_attn.flatten().any().item()
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1326, in <graph break in forward>
    layer_outputs = layer_module(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1249, in forward
    self_attn_outputs = self.attention(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1185, in forward
    self_outputs = self.self(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 574, in forward
    attn_scores = self._sliding_chunks_query_key_matmul(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 821, in _sliding_chunks_query_key_matmul
    def _sliding_chunks_query_key_matmul(self, query: torch.Tensor, key: torch.Tensor, window_overlap: int):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 250, in nvfuser_execute
    fusion, unflatten_spec = make_nvfuser_fusion(gm, *nv_template_args)  # type: ignore[misc]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 152, in make_nvfuser_fusion
    with FusionDefinition(fusion) as fd:
TypeError: __exit__(): incompatible function arguments. The following argument types are supported:
    1. (self: nvfuser._C.FusionDefinition, arg0: capsule, arg1: capsule, arg2: capsule) -> None

Invoked with: <nvfuser._C.FusionDefinition object at 0x7f14b49a02b0>, <class 'AssertionError'>, AssertionError('output from codegen has to be tensor type\n\nWhile executing return (view, view_1, sub)\nOriginal traceback:\nNone'), <traceback object at 0x7f14b49c6580>
ERROR
WARNING:__main__:Running smaller batch size=4 for BartForCausalLM, orig batch_size=8
cuda eval  BartForCausalLM                     0.968x p=0.00
TIMING: entire_frame_compile:15.94015 backend_compile:13.88118
STATS: call_* op count: 482 | FakeTensorMode.__torch_dispatch__:39843 | FakeTensor.__torch_dispatch__:2334 | ProxyTorchDispatchMode.__torch_dispatch__:32140
Dynamo produced 14 graphs covering 482 ops with 7 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=2 for BartForConditionalGeneration, orig batch_size=4
cuda eval  BartForConditionalGeneration        0.241x p=0.00
TIMING: entire_frame_compile:38.93412 backend_compile:35.17141
STATS: call_* op count: 1258 | FakeTensorMode.__torch_dispatch__:101754 | FakeTensor.__torch_dispatch__:5711 | ProxyTorchDispatchMode.__torch_dispatch__:83310
Dynamo produced 1 graphs covering 1258 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=16 for BertForMaskedLM, orig batch_size=32
cuda eval  BertForMaskedLM                     1.186x p=0.00
TIMING: entire_frame_compile:15.7553 backend_compile:13.65258
STATS: call_* op count: 370 | FakeTensorMode.__torch_dispatch__:40339 | FakeTensor.__torch_dispatch__:2605 | ProxyTorchDispatchMode.__torch_dispatch__:32293
Dynamo produced 1 graphs covering 370 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=16 for BertForQuestionAnswering, orig batch_size=32
cuda eval  BertForQuestionAnswering            1.218x p=0.00
TIMING: entire_frame_compile:16.02115 backend_compile:14.22368
STATS: call_* op count: 377 | FakeTensorMode.__torch_dispatch__:40494 | FakeTensor.__torch_dispatch__:2578 | ProxyTorchDispatchMode.__torch_dispatch__:32255
Dynamo produced 1 graphs covering 377 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=4 for BlenderbotForCausalLM, orig batch_size=32
cuda eval  BlenderbotForCausalLM               0.715x p=0.00
TIMING: entire_frame_compile:31.26318 backend_compile:28.22885
STATS: call_* op count: 934 | FakeTensorMode.__torch_dispatch__:77225 | FakeTensor.__torch_dispatch__:4434 | ProxyTorchDispatchMode.__torch_dispatch__:62858
Dynamo produced 26 graphs covering 934 ops with 7 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForCausalLM, orig batch_size=256
cuda eval  BlenderbotSmallForCausalLM          0.933x p=0.00
TIMING: entire_frame_compile:11.22016 backend_compile:9.84514
STATS: call_* op count: 327 | FakeTensorMode.__torch_dispatch__:27220 | FakeTensor.__torch_dispatch__:1634 | ProxyTorchDispatchMode.__torch_dispatch__:21793
Dynamo produced 10 graphs covering 327 ops with 7 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForConditionalGeneration, orig batch_size=128
cuda eval  BlenderbotSmallForConditionalGeneration  0.958x p=0.00
TIMING: entire_frame_compile:26.86837 backend_compile:24.40452
STATS: call_* op count: 840 | FakeTensorMode.__torch_dispatch__:68186 | FakeTensor.__torch_dispatch__:3840 | ProxyTorchDispatchMode.__torch_dispatch__:55892
Dynamo produced 1 graphs covering 840 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=16 for CamemBert, orig batch_size=32
cuda eval  CamemBert                           1.172x p=0.00
TIMING: entire_frame_compile:16.16678 backend_compile:14.35794
STATS: call_* op count: 377 | FakeTensorMode.__torch_dispatch__:40599 | FakeTensor.__torch_dispatch__:2605 | ProxyTorchDispatchMode.__torch_dispatch__:32563
Dynamo produced 1 graphs covering 377 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=4 for DebertaForMaskedLM, orig batch_size=32
cuda eval  DebertaForMaskedLM                  ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 484, in forward_pass
    return mod(**inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1072, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 983, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 445, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 419, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS111{( ceilDiv(( T8.size[0] * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS112{1}, iS134{( ceilDiv(( ceilDiv(( 32 * 32 ), 4) ), 128) )}, iS135{128}, iS133{4} ] with word size 4 not possible due to invalid stride. Domain: iS135{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=8 for DebertaForQuestionAnswering, orig batch_size=32
cuda eval  DebertaForQuestionAnswering         ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 484, in forward_pass
    return mod(**inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1401, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 983, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 445, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 419, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS111{( ceilDiv(( T8.size[0] * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS112{1}, iS134{( ceilDiv(( ceilDiv(( 32 * 32 ), 4) ), 128) )}, iS135{128}, iS133{4} ] with word size 4 not possible due to invalid stride. Domain: iS135{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=1 for DebertaV2ForMaskedLM, orig batch_size=8
cuda eval  DebertaV2ForMaskedLM                ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 484, in forward_pass
    return mod(**inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1171, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1083, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 488, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 452, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS109{( ceilDiv(( 1 * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS110{1}, iS132{( ceilDiv(( ceilDiv(( 32 * 32 ), 2) ), 128) )}, iS133{128}, iS131{2} ] with word size 2 not possible due to invalid stride. Domain: iS133{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=2 for DebertaV2ForQuestionAnswering, orig batch_size=8
cuda eval  DebertaV2ForQuestionAnswering       ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 484, in forward_pass
    return mod(**inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1501, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1083, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 488, in forward
    attention_mask = self.get_attention_mask(attention_mask)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 452, in get_attention_mask
    def get_attention_mask(self, attention_mask):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 488, in nvfuser_execute_partitioned
    return nvfuser_execute(gm, *args, executor_parameters=executor_parameters)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 278, in nvfuser_execute
    fusion.execute(concrete_fusion_inputs),  # type: ignore[has-type]
RuntimeError: stride == cur_contig_stride || size == 1 || (still_rightmost && stride == 1) || (!still_rightmost && stride % word_size == 0) INTERNAL ASSERT FAILED at "../third_party/nvfuser/csrc/executor_utils.cpp":621, please report a bug to PyTorch. Vectorization of T8_g[ iS111{( ceilDiv(( T8.size[0] * ( 1 * ( ( ceilDiv(T8.size[2], 32) ) * ( ceilDiv(1, 32) ) ) ) ), 1) )}, iS112{1}, iS134{( ceilDiv(( ceilDiv(( 32 * 32 ), 4) ), 128) )}, iS135{128}, iS133{4} ] with word size 4 not possible due to invalid stride. Domain: iS135{128}, stride: 0
ERROR
WARNING:__main__:Running smaller batch size=128 for DistilBertForMaskedLM, orig batch_size=256
WARNING:__main__:Sequence Length not defined for DistilBertForMaskedLM. Choosing 128 arbitrarily
cuda eval  DistilBertForMaskedLM               0.667x p=0.00
TIMING: entire_frame_compile:9.94858 backend_compile:7.76632
STATS: call_* op count: 206 | FakeTensorMode.__torch_dispatch__:21384 | FakeTensor.__torch_dispatch__:1163 | ProxyTorchDispatchMode.__torch_dispatch__:17431
Dynamo produced 1 graphs covering 206 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=256 for DistilBertForQuestionAnswering, orig batch_size=512
WARNING:__main__:Sequence Length not defined for DistilBertForQuestionAnswering. Choosing 128 arbitrarily
cuda eval  DistilBertForQuestionAnswering      0.612x p=0.00
TIMING: entire_frame_compile:8.18472 backend_compile:7.36475
STATS: call_* op count: 214 | FakeTensorMode.__torch_dispatch__:21539 | FakeTensor.__torch_dispatch__:1136 | ProxyTorchDispatchMode.__torch_dispatch__:17393
Dynamo produced 1 graphs covering 214 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=16 for DistillGPT2, orig batch_size=32
cuda eval  DistillGPT2                         1.287x p=0.00
TIMING: entire_frame_compile:7.43412 backend_compile:6.43749
STATS: call_* op count: 330 | FakeTensorMode.__torch_dispatch__:18377 | FakeTensor.__torch_dispatch__:794 | ProxyTorchDispatchMode.__torch_dispatch__:13501
Dynamo produced 1 graphs covering 330 ops with 0 graph breaks (0 unique)
If you want to use `ElectraForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=32 for ElectraForCausalLM, orig batch_size=64
cuda eval  ElectraForCausalLM                  1.155x p=0.00
TIMING: entire_frame_compile:16.16621 backend_compile:14.32777
STATS: call_* op count: 375 | FakeTensorMode.__torch_dispatch__:40882 | FakeTensor.__torch_dispatch__:2640 | ProxyTorchDispatchMode.__torch_dispatch__:32618
Dynamo produced 4 graphs covering 375 ops with 3 graph breaks (2 unique)
WARNING:__main__:Running smaller batch size=64 for ElectraForQuestionAnswering, orig batch_size=128
cuda eval  ElectraForQuestionAnswering         1.350x p=0.00
TIMING: entire_frame_compile:15.32054 backend_compile:13.55001
STATS: call_* op count: 378 | FakeTensorMode.__torch_dispatch__:40760 | FakeTensor.__torch_dispatch__:2602 | ProxyTorchDispatchMode.__torch_dispatch__:32443
Dynamo produced 1 graphs covering 378 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=4 for GPT2ForSequenceClassification, orig batch_size=8
cuda eval  GPT2ForSequenceClassification       1.487x p=0.00
TIMING: entire_frame_compile:13.7424 backend_compile:12.05867
STATS: call_* op count: 644 | FakeTensorMode.__torch_dispatch__:34715 | FakeTensor.__torch_dispatch__:1531 | ProxyTorchDispatchMode.__torch_dispatch__:25557
Dynamo produced 1 graphs covering 644 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=16 for GoogleFnet, orig batch_size=32
cuda eval  GoogleFnet                          1.744x p=0.00
TIMING: entire_frame_compile:8.66086 backend_compile:7.68012
STATS: call_* op count: 209 | FakeTensorMode.__torch_dispatch__:20254 | FakeTensor.__torch_dispatch__:883 | ProxyTorchDispatchMode.__torch_dispatch__:17601
Dynamo produced 28 graphs covering 209 ops with 41 graph breaks (4 unique)
WARNING:__main__:Running smaller batch size=16 for LayoutLMForMaskedLM, orig batch_size=32
cuda eval  LayoutLMForMaskedLM                 1.180x SAME
TIMING: entire_frame_compile:17.80291 backend_compile:14.42117
STATS: call_* op count: 396 | FakeTensorMode.__torch_dispatch__:42165 | FakeTensor.__torch_dispatch__:3095 | ProxyTorchDispatchMode.__torch_dispatch__:32838
Dynamo produced 1 graphs covering 396 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=16 for LayoutLMForSequenceClassification, orig batch_size=32
cuda eval  LayoutLMForSequenceClassification   1.230x p=0.00
TIMING: entire_frame_compile:16.48557 backend_compile:14.44565
STATS: call_* op count: 394 | FakeTensorMode.__torch_dispatch__:41582 | FakeTensor.__torch_dispatch__:3063 | ProxyTorchDispatchMode.__torch_dispatch__:32326
Dynamo produced 1 graphs covering 394 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=16 for M2M100ForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for M2M100ForConditionalGeneration. Choosing 128 arbitrarily
cuda eval  M2M100ForConditionalGeneration      0.789x p=0.00
TIMING: entire_frame_compile:38.53766 backend_compile:34.51315
STATS: call_* op count: 1266 | FakeTensorMode.__torch_dispatch__:101319 | FakeTensor.__torch_dispatch__:5843 | ProxyTorchDispatchMode.__torch_dispatch__:81872
Dynamo produced 19 graphs covering 1266 ops with 13 graph breaks (4 unique)
WARNING:__main__:Running smaller batch size=4 for MBartForCausalLM, orig batch_size=8
cuda eval  MBartForCausalLM                    0.951x p=0.00
TIMING: entire_frame_compile:16.6271 backend_compile:14.4994
STATS: call_* op count: 481 | FakeTensorMode.__torch_dispatch__:39835 | FakeTensor.__torch_dispatch__:2337 | ProxyTorchDispatchMode.__torch_dispatch__:32139
Dynamo produced 14 graphs covering 481 ops with 7 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=2 for MBartForConditionalGeneration, orig batch_size=4
cuda eval  MBartForConditionalGeneration       0.889x p=0.00
TIMING: entire_frame_compile:40.85381 backend_compile:37.28894
STATS: call_* op count: 1263 | FakeTensorMode.__torch_dispatch__:102526 | FakeTensor.__torch_dispatch__:5731 | ProxyTorchDispatchMode.__torch_dispatch__:84122
Dynamo produced 1 graphs covering 1263 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=16 for MT5ForConditionalGeneration, orig batch_size=32
WARNING:__main__:Sequence Length not defined for MT5ForConditionalGeneration. Choosing 128 arbitrarily
cuda eval  MT5ForConditionalGeneration         0.912x p=0.00
TIMING: entire_frame_compile:19.15919 backend_compile:15.76804
STATS: call_* op count: 1173 | FakeTensorMode.__torch_dispatch__:52288 | FakeTensor.__torch_dispatch__:4420 | ProxyTorchDispatchMode.__torch_dispatch__:33244
Dynamo produced 1 graphs covering 1173 ops with 0 graph breaks (0 unique)
If you want to use `MegatronBertForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=4 for MegatronBertForCausalLM, orig batch_size=16
cuda eval  MegatronBertForCausalLM             1.132x p=0.00
TIMING: entire_frame_compile:29.77302 backend_compile:26.31951
STATS: call_* op count: 721 | FakeTensorMode.__torch_dispatch__:78568 | FakeTensor.__torch_dispatch__:5101 | ProxyTorchDispatchMode.__torch_dispatch__:62892
Dynamo produced 1 graphs covering 721 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=8 for MegatronBertForQuestionAnswering, orig batch_size=16
cuda eval  MegatronBertForQuestionAnswering    1.184x p=0.00
TIMING: entire_frame_compile:29.80622 backend_compile:26.64951
STATS: call_* op count: 724 | FakeTensorMode.__torch_dispatch__:78507 | FakeTensor.__torch_dispatch__:5074 | ProxyTorchDispatchMode.__torch_dispatch__:62723
Dynamo produced 1 graphs covering 724 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=64 for MobileBertForMaskedLM, orig batch_size=256
cuda eval  MobileBertForMaskedLM               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 484, in forward_pass
    return mod(**inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py", line 1067, in forward
    @add_start_docstrings_to_model_forward(MOBILEBERT_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 477, in nvfuser_execute_partitioned
    gm, is_partitioned = maybe_partition_graph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 422, in maybe_partition_graph
    partitioned_graph = partitioner.fuse_partitions(partitions)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py", line 217, in fuse_partitions
    return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 204, in fuse_by_partitions
    sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule(gm, sorted_nodes, submodule_name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 106, in fuse_as_graphmodule
    assert validate_partition(nodes), "Invalid partition, found dependency cycles"
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 69, in validate_partition
    if dfs_find_cycle(output_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  [Previous line repeated 968 more times]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 61, in dfs_find_cycle
    visited.add(node)
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR
WARNING:__main__:Running smaller batch size=128 for MobileBertForQuestionAnswering, orig batch_size=256
cuda eval  MobileBertForQuestionAnswering      ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 484, in forward_pass
    return mod(**inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/mobilebert/modeling_mobilebert.py", line 1363, in forward
    @add_start_docstrings_to_model_forward(MOBILEBERT_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 477, in nvfuser_execute_partitioned
    gm, is_partitioned = maybe_partition_graph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 422, in maybe_partition_graph
    partitioned_graph = partitioner.fuse_partitions(partitions)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py", line 217, in fuse_partitions
    return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 204, in fuse_by_partitions
    sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule(gm, sorted_nodes, submodule_name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 106, in fuse_as_graphmodule
    assert validate_partition(nodes), "Invalid partition, found dependency cycles"
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 69, in validate_partition
    if dfs_find_cycle(output_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  [Previous line repeated 968 more times]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 61, in dfs_find_cycle
    visited.add(node)
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR
WARNING:__main__:Running smaller batch size=2 for OPTForCausalLM, orig batch_size=4
cuda eval  OPTForCausalLM                      0.678x p=0.00
TIMING: entire_frame_compile:16.462 backend_compile:13.49772
STATS: call_* op count: 533 | FakeTensorMode.__torch_dispatch__:39638 | FakeTensor.__torch_dispatch__:2267 | ProxyTorchDispatchMode.__torch_dispatch__:31660
Dynamo produced 14 graphs covering 533 ops with 7 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=8 for PLBartForCausalLM, orig batch_size=16
cuda eval  PLBartForCausalLM                   0.972x p=0.00
TIMING: entire_frame_compile:8.19486 backend_compile:6.95795
STATS: call_* op count: 254 | FakeTensorMode.__torch_dispatch__:20955 | FakeTensor.__torch_dispatch__:1278 | ProxyTorchDispatchMode.__torch_dispatch__:16588
Dynamo produced 8 graphs covering 254 ops with 7 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=4 for PLBartForConditionalGeneration, orig batch_size=8
cuda eval  PLBartForConditionalGeneration      0.974x p=0.00
TIMING: entire_frame_compile:22.30924 backend_compile:17.92426
STATS: call_* op count: 658 | FakeTensorMode.__torch_dispatch__:55362 | FakeTensor.__torch_dispatch__:3744 | ProxyTorchDispatchMode.__torch_dispatch__:42635
Dynamo produced 10 graphs covering 658 ops with 10 graph breaks (4 unique)
WARNING:__main__:Running smaller batch size=32 for PegasusForCausalLM, orig batch_size=128
WARNING:__main__:Sequence Length not defined for PegasusForCausalLM. Choosing 128 arbitrarily
cuda eval  PegasusForCausalLM                  0.956x p=0.00
TIMING: entire_frame_compile:15.85539 backend_compile:14.17641
STATS: call_* op count: 478 | FakeTensorMode.__torch_dispatch__:39209 | FakeTensor.__torch_dispatch__:2245 | ProxyTorchDispatchMode.__torch_dispatch__:31757
Dynamo produced 16 graphs covering 478 ops with 8 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=32 for PegasusForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for PegasusForConditionalGeneration. Choosing 128 arbitrarily
cuda eval  PegasusForConditionalGeneration     0.998x p=0.01
TIMING: entire_frame_compile:38.49077 backend_compile:34.94038
STATS: call_* op count: 1244 | FakeTensorMode.__torch_dispatch__:101389 | FakeTensor.__torch_dispatch__:5727 | ProxyTorchDispatchMode.__torch_dispatch__:83055
Dynamo produced 7 graphs covering 1244 ops with 11 graph breaks (3 unique)
If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=16 for RobertaForCausalLM, orig batch_size=32
cuda eval  RobertaForCausalLM                  1.165x p=0.00
TIMING: entire_frame_compile:15.94297 backend_compile:14.15903
STATS: call_* op count: 381 | FakeTensorMode.__torch_dispatch__:40785 | FakeTensor.__torch_dispatch__:2605 | ProxyTorchDispatchMode.__torch_dispatch__:32673
Dynamo produced 1 graphs covering 381 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=16 for RobertaForQuestionAnswering, orig batch_size=32
cuda eval  RobertaForQuestionAnswering         1.235x p=0.00
TIMING: entire_frame_compile:15.26186 backend_compile:13.54149
STATS: call_* op count: 384 | FakeTensorMode.__torch_dispatch__:40724 | FakeTensor.__torch_dispatch__:2578 | ProxyTorchDispatchMode.__torch_dispatch__:32504
Dynamo produced 1 graphs covering 384 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=256 for Speech2Text2ForCausalLM, orig batch_size=1024
WARNING:__main__:Sequence Length not defined for Speech2Text2ForCausalLM. Choosing 128 arbitrarily
cuda eval  Speech2Text2ForCausalLM             1.020x SAME
TIMING: entire_frame_compile:7.84635 backend_compile:6.81213
STATS: call_* op count: 261 | FakeTensorMode.__torch_dispatch__:20375 | FakeTensor.__torch_dispatch__:1196 | ProxyTorchDispatchMode.__torch_dispatch__:16139
Dynamo produced 10 graphs covering 261 ops with 8 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=4 for T5ForConditionalGeneration, orig batch_size=8
cuda eval  T5ForConditionalGeneration          1.073x p=0.00
TIMING: entire_frame_compile:14.29779 backend_compile:11.75395
STATS: call_* op count: 798 | FakeTensorMode.__torch_dispatch__:37090 | FakeTensor.__torch_dispatch__:3116 | ProxyTorchDispatchMode.__torch_dispatch__:24025
Dynamo produced 1 graphs covering 798 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=4 for T5Small, orig batch_size=8
cuda eval  T5Small                             1.072x p=0.00
TIMING: entire_frame_compile:14.37356 backend_compile:11.81409
STATS: call_* op count: 798 | FakeTensorMode.__torch_dispatch__:37090 | FakeTensor.__torch_dispatch__:3116 | ProxyTorchDispatchMode.__torch_dispatch__:24025
Dynamo produced 1 graphs covering 798 ops with 0 graph breaks (0 unique)
WARNING:__main__:Running smaller batch size=32 for TrOCRForCausalLM, orig batch_size=64
cuda eval  TrOCRForCausalLM                    1.007x SAME
TIMING: entire_frame_compile:15.99462 backend_compile:14.21915
STATS: call_* op count: 481 | FakeTensorMode.__torch_dispatch__:39535 | FakeTensor.__torch_dispatch__:2337 | ProxyTorchDispatchMode.__torch_dispatch__:32067
Dynamo produced 14 graphs covering 481 ops with 7 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=8 for XGLMForCausalLM, orig batch_size=32
WARNING:__main__:Sequence Length not defined for XGLMForCausalLM. Choosing 128 arbitrarily
cuda eval  XGLMForCausalLM                     0.157x p=0.00
TIMING: entire_frame_compile:29.95942 backend_compile:26.97205
STATS: call_* op count: 998 | FakeTensorMode.__torch_dispatch__:77952 | FakeTensor.__torch_dispatch__:4364 | ProxyTorchDispatchMode.__torch_dispatch__:63615
Dynamo produced 28 graphs covering 998 ops with 8 graph breaks (5 unique)
WARNING:__main__:Running smaller batch size=8 for XLNetLMHeadModel, orig batch_size=16
cuda eval  XLNetLMHeadModel                    ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 484, in forward_pass
    return mod(**inputs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py", line 1357, in forward
    @add_start_docstrings_to_model_forward(XLNET_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2819, in forward
    return compiled_fn(full_args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1898, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1247, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1222, in g
    return f(*args)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/executor.py", line 25, in execute
    return nvfuser_execute_partitioned(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 477, in nvfuser_execute_partitioned
    gm, is_partitioned = maybe_partition_graph(
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/_prims/nvfuser_executor.py", line 422, in maybe_partition_graph
    partitioned_graph = partitioner.fuse_partitions(partitions)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/infra/partitioner.py", line 217, in fuse_partitions
    return fuse_by_partitions(self.graph_module, [list(partition.nodes) for partition in partitions])
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 204, in fuse_by_partitions
    sub_gm, orig_inputs, orig_outputs = fuse_as_graphmodule(gm, sorted_nodes, submodule_name)
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 106, in fuse_as_graphmodule
    assert validate_partition(nodes), "Invalid partition, found dependency cycles"
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 69, in validate_partition
    if dfs_find_cycle(output_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 64, in dfs_find_cycle
    if dfs_find_cycle(user_node):
  [Previous line repeated 968 more times]
  File "/data/home/voz/miniconda3/envs/benchmarks/lib/python3.10/site-packages/torch/fx/passes/utils/fuser_utils.py", line 61, in dfs_find_cycle
    visited.add(node)
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR
WARNING:__main__:Running smaller batch size=16 for YituTechConvBert, orig batch_size=32
cuda eval  YituTechConvBert                    1.071x p=0.00
TIMING: entire_frame_compile:24.6464 backend_compile:19.81927
STATS: call_* op count: 634 | FakeTensorMode.__torch_dispatch__:60806 | FakeTensor.__torch_dispatch__:3804 | ProxyTorchDispatchMode.__torch_dispatch__:45837
Dynamo produced 4 graphs covering 634 ops with 3 graph breaks (2 unique)
speedup             gmean=1.09x mean=1.098x
abs_latency         gmean=nanx mean=50.116x
compilation_latency mean=22.025 seconds
compression_ratio   mean=0.993x
eager_peak_mem      gmean=nanx mean=4.062x
dynamo_peak_mem     gmean=nanx mean=4.164x
calls_captured      gmean=nanx mean=592.605x
unique_graphs       gmean=nanx mean=6.526x
graph_breaks        gmean=nanx mean=4.632x
unique_graph_breaks gmean=nanx mean=2.395x
